{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timings and stencils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment is meant to be run on a node with 28 cores.\n",
    "\n",
    "One thing we are trying to do is gauge the performance of a good MPI implementation and compare it to a good OpenMP implementation.  In order to do this, we will have to load some slightly different modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " =========================================================================\n",
      "|                                                                         |\n",
      "|       A note about python/3.6:                                          |\n",
      "|       PACE is lacking the staff to install all of the python 3          |\n",
      "|       modules, but we do maintain an anaconda distribution for          |\n",
      "|       both python 2 and python 3. As conda significantly reduces        |\n",
      "|       the overhead with package management, we would much prefer        |\n",
      "|       to maintain python 3 through anaconda.                            |\n",
      "|                                                                         |\n",
      "|       All pace installed modules are visible via the module avail       |\n",
      "|       command.                                                          |\n",
      "|                                                                         |\n",
      " =========================================================================\n",
      "Currently Loaded Modulefiles:\n",
      "  1) curl/7.42.1\n",
      "  2) hwloc/1.10.0(default)\n",
      "  3) git/2.13.4\n",
      "  4) python/3.6\n",
      "  5) /nv/coc-ice/tisaac3/opt/pace-ice/modulefiles/jupyter/1.0\n",
      "  6) intel/17.0\n",
      "  7) mvapich2/2.3\n",
      "  8) cse6230/mvapich\n"
     ]
    }
   ],
   "source": [
    "module unload cse6230\n",
    "module load cse6230/mvapich\n",
    "module list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "([mvapich](http://mvapich.cse.ohio-state.edu/) is a fork of the [mpich](https://www.mpich.org/) MPI implementation with some modifications for performance on various HPC hardware)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring MPI primitives\n",
    "\n",
    "**Task 1 (3 pts)** The file `benchmarks.c` includes some basic benchmarks for measuring the performance of various MPI point-to-point and collective operations.  Right now, it is incomplete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  /* TODO: split the communicator `comm` into one communicator for ranks\n",
      "  /* TODO: destroy the subcommunicator created in `splitCommunicator` */\n",
      "  /* TODO: insert a barrier on `comm` to synchronize the processes */\n",
      "  /* TODO: Record the MPI walltime in `tic_p` */\n",
      "  /* TODO: Get the elapsed MPI walltime since `tic_in`,\n",
      "  /* TODO: take the times from all processes and compute the maximum,\n",
      "          /* TODO: Use the subComm communicator to broadcast from rank 0 the\n",
      "          /* TODO: Use the subComm communicator to scatter from rank 0 the first\n",
      "          /* TODO: Use the subComm communicator to compute the minimum of the\n",
      "          /* TODO: Use the subComm communicator to gather the first `numBytes`\n",
      "          /* TODO: Use the subComm communicator to sum the first `numBytes`\n",
      "          /* TODO: Use the subComm communicator to gather the first `numBytes`\n",
      "          /* TODO: Use the subComm communicator to transpose the first\n"
     ]
    }
   ],
   "source": [
    "grep \"TODO\" benchmarks.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refering to a good [MPI Tutorial](https://computing.llnl.gov/tutorials/mpi/) or\n",
    "[lecture notes](http://vuduc.org/cse6230/slides/cse6230-fa14--06-mpi.pdf) as needed,\n",
    "fill in the missing MPI routines.\n",
    "\n",
    "Once you have done that, run the following script to generate a graph of benchmark bandwidths of MPI routines.  Note that these values are only for MPI messages within a node: values may be different when we start using multiple nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpicc -qopenmp -I../../utils -g -Wall -O3 -xHost -std=c99 -c -o benchmarks.o benchmarks.c\n",
      "gcc: unrecognized option '-qopenmp'\n",
      "gcc: language Host not recognized\n",
      "gcc: language Host not recognized\n",
      "gcc: benchmarks.c: linker input file unused because linking not done\n",
      "mpicc -qopenmp -o benchmarks benchmarks.o \n",
      "gcc: benchmarks.o: No such file or directory\n",
      "gcc: unrecognized option '-qopenmp'\n",
      "make: *** [benchmarks] Error 1\n"
     ]
    },
    {
     "ename": "",
     "evalue": "2",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "make runbenchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bash: benchmarks.png: No such file or directory\n"
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "display < benchmarks.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Right now the graph is showing no values because the \"timing\" values are negative until you complete the code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2 (2 pts)** We've talked in class about a simplified model of the cost of an MPI message: $\\lambda + g b$, where $\\lambda$ is the latency and $g$ is the inverse bandwidth.\n",
    "Using your graph for Send/Recv bandwidths for different message sizes (which was simply calculated from dividing the message size by the message time), estimate $\\lambda$ (units secs) and $g$ (units secs/byte) for this MPI implementation on this node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From particles to stencils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project 1 showed one way to deal with the $O(n^2)$ bottleneck for computing $n$-body calculations directly: modify the potential so that it acts locally.\n",
    "\n",
    "In this assignment, we consider a different approach.  The common inverse-square-law potential $\\phi$ generated by a particle at location $x$ solves (in a certain sense) a partial differential equation (PDE):\n",
    "\n",
    "$$-\\Delta \\phi = \\delta(x),$$\n",
    "\n",
    "Where the differential operator on the right is the Laplacian, and $\\delta(x)$ indicates a point mass (or point charge, depending on the setting).  The PDE is linear, so the potential due to all of the particles can be written as\n",
    "\n",
    "$$-\\Delta \\phi = \\sum_p \\delta(x_p).$$\n",
    "\n",
    "PDEs, if you are not familiar, can be approximately solved very efficiently on a regular grid, where the action of $\\Delta$ is approximated by a stencil operation.  A strategy for evolving the $n$-body problem can thus take the following form:\n",
    "\n",
    "1. Approximate the sum of delta functions $\\sum_p \\delta(x_p)$ as a grid function $f$ by\n",
    "   adding the mass/charge of each particle to one or more points in the grid.  (This is a\n",
    "   local operation and is not difficult to make efficient).\n",
    "\n",
    "2. Solve $-\\Delta \\phi = f$.  (This is where all of the effort is.)\n",
    "\n",
    "3. The acceleration due to the potential at a point $x$ is approximately $\\nabla \\phi (x)$.\n",
    "\n",
    "Therefore numerical methods, like the Jacobi smoother that we discussed in the context of cache-blocking for stencils, can be employed for $n$-body simulations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've written a small code that implements a 3D Jacobi smoother.  The applies a smoother `NT` times on a grid of size `BX` by `BY` by `BZ` (the $z$ direction is the fastest varying, that is, the leftmost index in FORTRAN or the rightmost index in C) that can be run three ways:\n",
    "\n",
    "1. As a pure MPI code.  Here `MPI_N` is the total number of MPI processes,\n",
    "`MPI_X`, `MPI_Y`, and `MPI_Z` are the number of processes in each direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OMP_NUM_THREADS=1 OMP_PROC_BIND=spread mpiexec -bind-to core -n 8 ./poisson 2 2 2 1 1 1 4 4 4 1 1000\n",
      "[2 x 2 x 2] processes, [1 x 1 x 1] threads per process, [4 x 4 x 4] boxes, 1 particles, 1000 smoother steps\n",
      "Sweep time: 0.00609207 seconds (0.00572658 seconds communication)\n",
      "Rate: 1.05055e+07 lattice updates per second\n",
      "Rate: 174624 halo exchanges per second\n"
     ]
    }
   ],
   "source": [
    "make runpoisson MPI_N=8 MPI_X=2 MPI_Y=2 MPI_Z=2 BX=4 BY=4 BZ=4 NT=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. As a pure openMP code, with `OMP_N`, `OMP_X`, `OMP_Y`, and `OMP_Z` options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OMP_NUM_THREADS=8 OMP_PROC_BIND=true ./poisson 1 1 1 2 2 2 4 4 4 1 1000\n",
      "[1 x 1 x 1] processes, [2 x 2 x 2] threads per process, [4 x 4 x 4] boxes, 1 particles, 1000 smoother steps\n",
      "Sweep time: 0.00851107 seconds (0.00311232 seconds communication)\n",
      "Rate: 7.51962e+06 lattice updates per second\n",
      "Rate: 321304 halo exchanges per second\n"
     ]
    }
   ],
   "source": [
    "make runpoissonomp OMP_N=8 OMP_X=2 OMP_Y=2 OMP_Z=2 BX=4 BY=4 BZ=4 NT=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. As a hybrid MPI+openMP code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OMP_NUM_THREADS=8 OMP_PROC_BIND=spread mpiexec -bind-to core -n 8 ./poisson 2 2 2 2 2 2 8 8 8 1 10\n",
      "[2 x 2 x 2] processes, [2 x 2 x 2] threads per process, [8 x 8 x 8] boxes, 1 particles, 10 smoother steps\n",
      "Sweep time: 0.162711 seconds (0.145163 seconds communication)\n",
      "Rate: 31466.8 lattice updates per second\n",
      "Rate: 68.888 halo exchanges per second\n"
     ]
    }
   ],
   "source": [
    "make runpoisson MPI_N=8 MPI_X=2 MPI_Y=2 MPI_Z=2 OMP_N=8 OMP_X=2 OMP_Y=2 OMP_Z=2 BX=8 BY=8 BZ=8 NT=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explore whether these implementations match our expectations for how quickly a Jacobi smoother can be applied, and whether there is any significant performance difference between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3 (2 pts)** We measure the performance of a stencil code in *Lattice updates per second*: a measure of how quickly we can process a grid point.  Our Jacobi smoother has two steps at each grid point.\n",
    "\n",
    "1. Apply the smoother to the 7-point stencil: $v_{i,j,k} \\leftarrow \\frac{1}{6} (f_{i,j,k} + u_{i\\pm 1, j\\pm 1, k\\pm 1})$, where $(i\\pm 1, j\\pm 1, k\\pm 1)$ is every grid point that differs from $(i,j,k)$ in one direction (there are six).\n",
    "2. Copy $u_{i,j,k} \\leftarrow v_{i,j,k}$\n",
    "\n",
    "It looks like there are 7 reads and one write in the first loop, but if the size of the array in each direction is small enough for a [layer condition](https://rrze-hpc.github.io/layer-condition/), then only one value of $u$ must be read instead of six.\n",
    "The best memory traffic we can hope for then, is two reads ($f$ and $u$) and one write ($v$) per grid point in the first loop, an one read ($v$) and one write ($u$) per grid point in the second loop, for a total of 5 doubles, or **40 bytes per grid point**.\n",
    "\n",
    "In our assignment on the [stream benchmark](https://www.cs.virginia.edu/stream/),\n",
    "we estimated the peak bandwidh of this type of node at about **100 Gbytes / sec**.\n",
    "\n",
    "Using these data points, estimate an upper bound on how many lattice updates per second (LUPs) our Jacobi smoother can accomplish on a large array that doesn't fit in cache.  Give that value in the cell below, and find a way of calling the MPI version of the code that\n",
    "\n",
    "- Runs on a big enough problem (You may want to use `lstopo` or similar)\n",
    "- Runs for about a second\n",
    "- Gets close to the theoretical LUPs predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpicc -qopenmp -I../../utils -g -Wall -O3 -xHost -std=c99 -c -o poisson.o poisson.c\n",
      "mpicc -qopenmp -o poisson poisson.o jacobi.o cloud_util.o \n",
      "OMP_NUM_THREADS=1 OMP_PROC_BIND=spread mpiexec -bind-to core -n 8 ./poisson 2 2 2 1 1 1 4 4 4 1 1000\n",
      "[2 x 2 x 2] processes, [1 x 1 x 1] threads per process, [4 x 4 x 4] boxes, 1 particles, 1000 smoother steps\n",
      "Sweep time: 0.00614905 seconds (0.0057888 seconds communication)\n",
      "Rate: 1.04081e+07 lattice updates per second\n",
      "Rate: 21593.4 halo exchanges per MPI process per second\n"
     ]
    }
   ],
   "source": [
    "make runpoisson MPI_N=8 MPI_X=2 MPI_Y=2 MPI_Z=2 BX=4 BY=4 BZ=4 NT=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4 (2 pts)** What is the most LUPs you can achieve on this node, using either the MPI or OpenMP code?  Give that command below.\n",
    "\n",
    "(I give you the choice of using MPI or OpenMP, but you may find it challenging to find an example where their performance is signficantly different.  As I said, you can accomplish just about the same things with both tools, the more important question is what is the best design for your code: threads or processes.)\n",
    "\n",
    "If you choose a problem small enough to fit in cache, you will be able to do better than\n",
    "the rate from the previous task.  But too small and the overhead of parallelization will outweigh the benefits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OMP_NUM_THREADS=8 OMP_PROC_BIND=true ./poisson 1 1 1 2 2 2 4 4 4 1 1000\n",
      "[1 x 1 x 1] processes, [2 x 2 x 2] threads per process, [4 x 4 x 4] boxes, 1 particles, 1000 smoother steps\n",
      "Sweep time: 0.00890303 seconds (0.00302553 seconds communication)\n",
      "Rate: 7.18857e+06 lattice updates per second\n",
      "Rate: 330520 halo exchanges per second\n"
     ]
    }
   ],
   "source": [
    "make runpoissonomp OMP_N=8 OMP_X=2 OMP_Y=2 OMP_Z=2 BX=4 BY=4 BZ=4 NT=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5 (1 pt)** If a halo exchange (when the subgrid of an MPI process exchanges its boundary values with its neighbors) consists of 6 `MPI_Sendrecv`s (two in each direction),\n",
    "then what does your $\\lambda$ value above predict is the most halo exchanges per second that can be achieved per MPI process per second?  Give a command where you try to maximize the number of halo exchanges per MPI process per second below (using all 28 MPI processes)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
