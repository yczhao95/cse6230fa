{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Performance Metrics & First Week Flop/s\n",
    "\n",
    "This assignment has some questions that you need to answer with text, and some code that you need to write.\n",
    "\n",
    "You should put all of you textual answers in this notebook: `Insert->Insert Cell Below` to create a new cell below\n",
    "the question, and `Cell->Cell Type->Markdown` to make it a cell for entering text.\n",
    "\n",
    "You will test your code on the compute nodes of pace-ice, and that it also where we will evaluate it.\n",
    "Please complete the text portions when you are logged into a head node working locally, and leave the compute nodes for when you actually need them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics\n",
    "\n",
    "In class we talked about the _strong-scaling efficiency_ of a parallel algorithm / machine pair: $H_f(P) = T_f(1) / (P T_f(P))$.\n",
    "\n",
    "We then talked about the _weak-scaling efficiency_ of algorithm $f$ that can be applied to different problem sizes $N$: $E_f(N,P) = T_f(N/P,1) / T_f(N,P)$.\n",
    "\n",
    "The question came up of how they are related to each other.\n",
    "\n",
    "First, the notion of strong scaling doesn't have a concept of problem size, so let's add it: let's define\n",
    "\n",
    "$$H_f(N,P) = T_f(N,1) / (P T_f(N,P)).$$\n",
    "\n",
    "This is simply strong-scaling efficiency for each problem instance individually.\n",
    "\n",
    "**Question 1 (1 pt):** Show that the relative order of strong and weak scaling efficiency can be related to the efficiency of the serial algorithm, that is, whether $T_f(N,1)$ as a function of $N$ exhibits superlinear or sublinear behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PACE-ICE\n",
    "\n",
    "**Head node exercise 1 (1 pt):** What command should you run from a head node to see a list of all the compute nodes and their availability?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <-- Put your command there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it out: open up this notebook on a head node and compare the list you get to the [orientation slides](http://pace.gatech.edu/sites/default/files/pace-ice_orientation_0.pdf).  You'll see that it has grown, and they haven't updated the orientation slides.  We'll just have to find out what all these new nodes are for ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Head node exercise 2 (1 pt):** For the next questions, I need you to log in to compute nodes to find out about them, but I need to be able to specify which type of compute nodes you are accessing.\n",
    "\n",
    "For each of the types of nodes that you see in the list of resources you've created, give me the `qsub` command to start an interactive job on that type of node, with the following requirements:\n",
    "\n",
    "* The job should give you exclusive access to one node and all its cores and devices.\n",
    "* The job should let you pop open an X window (like a notebook) if you want to.\n",
    "* The job should begin in the CSE6230 directory.\n",
    "* The job should end after 30 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put one qsub command in this cell, and duplicate the cell for the others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What have we got to work with?\n",
    "\n",
    "Now, we need to switch from a notebook running on the head node to one running on a compue node, so `File->Save and Checkpoint` this notebook and `File->Close and Halt` it.  (Now would also be a good time to `git add` and `git commit` changes to this file.)  Use one of your ineractive job scripts to connect to a compute node and run the notebook there.\n",
    "See you on the other side!\n",
    "\n",
    "---\n",
    "\n",
    "Okay, you're running on the compute node.\n",
    "\n",
    "**Compute node exercise 1 (1 pt):** Using bash scripting (`awk`, `grep`, `sed`) or any other tool you like (you could, e.g., write a python script in a separate file and call it, as long as you `git add` it), set the following variables so that the printout that follows is correct.  You script should be correct on any type of compute node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "CPU_NAME=\n",
    "CORE_COUNT=\n",
    "GPU_NAME=\n",
    "GPU_COUNT="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This nodes has  cores: its architecture is (Manufacturer, Product Id) \n",
      "This node has no GPUs\n"
     ]
    }
   ],
   "source": [
    "echo \"This nodes has ${CORE_COUNT} cores: its architecture is (Manufacturer, Product Id) ${CPU_NAME}\"\n",
    "if [[ ! $GPU_COUNT || $GPU_COUNT == 0 ]] ;  then\n",
    "    echo \"This node has no GPUs\"\n",
    "else\n",
    "    echo \"This node has ${GPU_COUNT} GPUs: its/their architecture is (Manufacturer, Product Id) ${GPU_NAME}\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute node exercise 2 (1 pt):** After you have logged out of the compute node, use whatever resources published on the web you can find to estimate the peak single precision flop/s of this node (you only need to do this step for one of the types of nodes, not all of them)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flop/s fever\n",
    "\n",
    "We've just got to scratch that itch: we just want to go fast.  Okay, let's get it out of our system, and we'll look at more practical computations in future assignments.\n",
    "\n",
    "**Compute node exercise 3 (3 pts):** The command below will compile and run the following computation:\n",
    "\n",
    "```C\n",
    "for (i = 0; i < N; i++) {\n",
    "  for (j = 0; j < T; j++) {\n",
    "    a[i] = a[i] * b + c;\n",
    "  }\n",
    "}\n",
    "```\n",
    "And it will report the flop/s for the whole calculation.\n",
    "\n",
    "Except that the array `a` will be spread out: `Nh` entries will be on the host and `Nd` entries will be on each of the devices.  Try to find values of `Nh`, `Nd`, and `T`, and (optionally) compiler optimization flags that give you the highest flop/s.  Things to consider:\n",
    "\n",
    "- Try to make your whole computation run for about a second.\n",
    "- The time reported is the maximum time for any device: if one sits idle while the other finishes, it will rob you of flop/s.\n",
    "- I suggest looking at one type of device at a time: set one of `Nh` or `Nd` to zero.  Once you've found your best flop/s for that device, optimize the other, and then try to strike a balance.\n",
    "- Experiment with the merits of putting more weight on `Nh` and `Nd` vs more weight on `T`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "icc -g -Wall -std=c99 -fPIC -O3 -xHost -qopt-report=5 -I/usr/local/pacerepov1/cuda/8.0.44/include -qopenmp -c -o fma_prof.o fma_prof.c\n",
      "icc: remark #10397: optimization reports are generated in *.optrpt files in the output location\n",
      "icc -g -Wall -std=c99 -fPIC -O3 -xHost -qopt-report=5 -I/usr/local/pacerepov1/cuda/8.0.44/include -qopenmp -c -o fma_omp.o fma_omp.c\n",
      "icc: remark #10397: optimization reports are generated in *.optrpt files in the output location\n",
      "icc -g -Wall -std=c99 -fPIC -O3 -xHost -qopt-report=5 -I/usr/local/pacerepov1/cuda/8.0.44/include -qopenmp -c -o fma_loop_host.o fma_loop_host.c\n",
      "icc: remark #10397: optimization reports are generated in *.optrpt files in the output location\n",
      "nvcc -ccbin=icpc -G -Xcompiler '-fPIC'  -dc -o fma_cuda.o fma_cuda.cu\n",
      "nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "nvcc -ccbin=icpc -G -Xcompiler '-fPIC'  -dc -o fma_loop_dev.o fma_loop_dev.cu\n",
      "nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "nvcc -ccbin=icpc -G -Xcompiler '-fPIC' -dlink  fma_cuda.o fma_loop_dev.o -o fma_cuda_link.o\n",
      "nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "icpc -qopenmp -shared -Wl,-soname,libfma_cuda.so -o libfma_cuda.so fma_cuda_link.o fma_cuda.o fma_loop_dev.o -L/usr/local/pacerepov1/cuda/8.0.44/lib64 -Wl,-rpath,/usr/local/pacerepov1/cuda/8.0.44/lib64 -lcudart\n",
      "icpc -qopenmp -o fma_prof fma_prof.o fma_omp.o fma_loop_host.o libfma_cuda.so\n",
      "icc -g -Wall -std=c99 -fPIC -O3 -xHost -qopt-report=5 -I/usr/local/pacerepov1/cuda/8.0.44/include -qopenmp -c -o fma_loop_host_opt.o fma_loop_host_opt.c\n",
      "icc: remark #10397: optimization reports are generated in *.optrpt files in the output location\n",
      "nvcc -ccbin=icpc -G -Xcompiler '-fPIC'  -dc -o fma_loop_dev_opt.o fma_loop_dev_opt.cu\n",
      "nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "nvcc -ccbin=icpc -G -Xcompiler '-fPIC' -dlink  fma_cuda.o fma_loop_dev_opt.o -o fma_cuda_link_opt.o\n",
      "nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "icpc -qopenmp -shared -Wl,-soname,libfma_cuda_opt.so -o libfma_cuda_opt.so fma_cuda_link_opt.o fma_cuda.o fma_loop_dev_opt.o -L/usr/local/pacerepov1/cuda/8.0.44/lib64 -Wl,-rpath,/usr/local/pacerepov1/cuda/8.0.44/lib64 -lcudart\n",
      "icpc -qopenmp -o fma_prof_opt fma_prof.o fma_omp.o fma_loop_host_opt.o libfma_cuda_opt.so\n",
      "OMP_PROC_BIND=spread OMP_NUM_THREADS=12 perf stat -v ./fma_prof 256 256 256 0.5 3.0\n",
      "Warning:\n",
      "stalled-cycles-frontend event is not supported by the kernel.\n",
      "Warning:\n",
      "stalled-cycles-backend event is not supported by the kernel.\n",
      "numDevices 2\n",
      "\n",
      "[./fma_prof]: 1.244068e-03 elapsed seconds\n",
      "\n",
      "\n",
      "[./fma_prof]: 393216 flops executed\n",
      "\n",
      "\n",
      "[./fma_prof]: 3.160727e+08 flop/s\n",
      "\n",
      "task-clock: 623241439 623241439 623241439\n",
      "context-switches: 402 623241439 623241439\n",
      "cpu-migrations: 33 623241439 623241439\n",
      "page-faults: 6563 623241439 623241439\n",
      "cycles: 1785331412 623996335 623996335\n",
      "stalled-cycles-frontend: 0 0 0\n",
      "stalled-cycles-backend: 0 0 0\n",
      "instructions: 1098811899 623996335 623996335\n",
      "branches: 246779048 623996335 623996335\n",
      "branch-misses: 1226117 623996335 623996335\n",
      "\n",
      " Performance counter stats for './fma_prof 256 256 256 0.5 3.0':\n",
      "\n",
      "        623.241439      task-clock (msec)         #    1.056 CPUs utilized          \n",
      "               402      context-switches          #    0.645 K/sec                  \n",
      "                33      cpu-migrations            #    0.053 K/sec                  \n",
      "             6,563      page-faults               #    0.011 M/sec                  \n",
      "     1,785,331,412      cycles                    #    2.865 GHz                    \n",
      "   <not supported>      stalled-cycles-frontend  \n",
      "   <not supported>      stalled-cycles-backend   \n",
      "     1,098,811,899      instructions              #    0.62  insns per cycle        \n",
      "       246,779,048      branches                  #  395.961 M/sec                  \n",
      "         1,226,117      branch-misses             #    0.50% of all branches        \n",
      "\n",
      "       0.590403285 seconds time elapsed\n",
      "\n",
      "OMP_PROC_BIND=spread OMP_NUM_THREADS=12 perf stat -v ./fma_prof_opt 256 256 256 0.5 3.0\n",
      "Warning:\n",
      "stalled-cycles-frontend event is not supported by the kernel.\n",
      "Warning:\n",
      "stalled-cycles-backend event is not supported by the kernel.\n",
      "numDevices 2\n",
      "\n",
      "[./fma_prof_opt]: 1.251936e-03 elapsed seconds\n",
      "\n",
      "\n",
      "[./fma_prof_opt]: 393216 flops executed\n",
      "\n",
      "\n",
      "[./fma_prof_opt]: 3.140864e+08 flop/s\n",
      "\n",
      "task-clock: 607887504 607887504 607887504\n",
      "context-switches: 278 607887504 607887504\n",
      "cpu-migrations: 35 607887504 607887504\n",
      "page-faults: 5900 607887504 607887504\n",
      "cycles: 1729462215 608330326 608330326\n",
      "stalled-cycles-frontend: 0 0 0\n",
      "stalled-cycles-backend: 0 0 0\n",
      "instructions: 1026006122 608330326 608330326\n",
      "branches: 244154364 608330326 608330326\n",
      "branch-misses: 990439 608330326 608330326\n",
      "\n",
      " Performance counter stats for './fma_prof_opt 256 256 256 0.5 3.0':\n",
      "\n",
      "        607.887504      task-clock (msec)         #    1.192 CPUs utilized          \n",
      "               278      context-switches          #    0.457 K/sec                  \n",
      "                35      cpu-migrations            #    0.058 K/sec                  \n",
      "             5,900      page-faults               #    0.010 M/sec                  \n",
      "     1,729,462,215      cycles                    #    2.845 GHz                    \n",
      "   <not supported>      stalled-cycles-frontend  \n",
      "   <not supported>      stalled-cycles-backend   \n",
      "     1,026,006,122      instructions              #    0.59  insns per cycle        \n",
      "       244,154,364      branches                  #  401.644 M/sec                  \n",
      "           990,439      branch-misses             #    0.41% of all branches        \n",
      "\n",
      "       0.510182052 seconds time elapsed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "make run_fma_prof Nh=256 Nd=256 T=256 COPTFLAGS='-O -xHost' CUOPTFLAGS='-O'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute Node Exercise 4 (2 pts):** Now let's see if we can make any transformations to the code to make a difference.\n",
    "\n",
    "We will run the same program, but with fused multiply add loops that you have try to optimize.  You should edit the files\n",
    "`fma_loop_host_opt.cu` and/or `fma_loop_dev_opt.c`: they start out exactly the same as the reference implementations used above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff fma_loop_host.c fma_loop_host_opt.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff fma_loop_dev.cu fma_loop_dev_opt.cu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See if you can exploit vectorization, instruction level parallelism, and/or loop transformations to get a boost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make run_fma_prof_opt Nh=256 Nd=256 T=256 COPTFLAGS='-O -xHost' CUOPTFLAGS='-O'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
