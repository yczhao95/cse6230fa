{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Performance Metrics & First Week Flop/s\n",
    "\n",
    "This assignment has some questions that you need to answer with text, and some code that you need to write.\n",
    "\n",
    "You should put all of you textual answers in this notebook: `Insert->Insert Cell Below` to create a new cell below\n",
    "the question, and `Cell->Cell Type->Markdown` to make it a cell for entering text.\n",
    "\n",
    "You will test your code on the compute nodes of pace-ice, and that it also where we will evaluate it.\n",
    "Please complete the text portions when you are logged into a head node working locally, and leave the compute nodes for when you actually need them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics\n",
    "\n",
    "In class we talked about the _strong-scaling efficiency_ of a parallel algorithm / machine pair: $H_f(P) = T_f(1) / (P T_f(P))$.\n",
    "\n",
    "We then talked about the _weak-scaling efficiency_ of algorithm $f$ that can be applied to different problem sizes $N$: $E_f(N,P) = T_f(N/P,1) / T_f(N,P)$.\n",
    "\n",
    "The question came up of how they are related to each other.\n",
    "\n",
    "First, the notion of strong scaling doesn't have a concept of problem size, so let's add it: let's define\n",
    "\n",
    "$$H_f(N,P) = T_f(N,1) / (P T_f(N,P)).$$\n",
    "\n",
    "This is simply strong-scaling efficiency for each problem instance individually.\n",
    "\n",
    "**Question 1 (1 pt):** Show that the relative order of strong and weak scaling efficiency can be related to the efficiency of the serial algorithm, that is, whether $T_f(N,1)$ as a function of $N$ exhibits superlinear or sublinear behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PACE-ICE\n",
    "\n",
    "**Head node exercise 1 (1 pt):** What command should you run from a head node to see a list of all the compute nodes and their availability?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== coc-ice Queue Summary: ====\n",
      "\tLast Update                            : 08/30/2018 02:15:02\n",
      "\tNumber of Nodes (Accepting Jobs/Total) : 42/42 (100.00%)\n",
      "\tNumber of Cores (Used/Total)           : 3/720 ( 0.42%)\n",
      "\tAmount of Memory (Used/Total) (MB)     : 92707/5636623 ( 1.64%)\n",
      "=================================================================================\n",
      "  Hostname       tasks/np Cpu%  loadav%  used/totmem(MB)   Mem%   Accepting Jobs? \n",
      "=================================================================================\n",
      "rich133-k33-17     3/8    37.5     3.6      3782/260408     1.5    Yes (free)             \n",
      "rich133-k40-17     0/8     0.0     2.1      2740/131128     2.1    Yes (free)             \n",
      "rich133-k40-18     0/8     0.0     0.2      3099/131128     2.4    Yes (free)             \n",
      "rich133-k40-20-l   0/28    0.0     0.3      3233/131126     2.5    Yes (free)             \n",
      "rich133-k40-20-r   0/28    0.0     0.1      3110/131126     2.4    Yes (free)             \n",
      "rich133-k40-21-l   0/28    0.0     0.9      2824/131126     2.2    Yes (free)             \n",
      "rich133-k40-21-r   0/28    0.0     0.5      2886/131126     2.2    Yes (free)             \n",
      "rich133-k40-22-l   0/28    0.0     0.7      2824/131126     2.2    Yes (free)             \n",
      "rich133-k40-22-r   0/28    0.0     0.7      3130/131126     2.4    Yes (free)             \n",
      "rich133-k40-23-l   0/28    0.0     0.0      3076/131126     2.3    Yes (free)             \n",
      "rich133-k40-23-r   0/28    0.0     0.7      2778/131126     2.1    Yes (free)             \n",
      "rich133-k40-24-l   0/28    0.0     0.1      2786/131126     2.1    Yes (free)             \n",
      "rich133-k40-24-r   0/28    0.0     1.0      3063/131126     2.3    Yes (free)             \n",
      "rich133-k40-25-l   0/28    0.0     0.5      2926/131126     2.2    Yes (free)             \n",
      "rich133-k40-25-r   0/28    0.0     2.0      2756/131126     2.1    Yes (free)             \n",
      "rich133-k40-26-l   0/28    0.0     0.2      3134/131126     2.4    Yes (free)             \n",
      "rich133-k40-26-r   0/28    0.0     0.0      2809/131126     2.1    Yes (free)             \n",
      "rich133-k40-27-l   0/28    0.0     0.8      2824/131126     2.2    Yes (free)             \n",
      "rich133-k40-27-r   0/28    0.0     0.9      3042/131126     2.3    Yes (free)             \n",
      "rich133-k40-29     0/8     0.0     0.0      2685/131128     2.0    Yes (free)             \n",
      "rich133-k40-30     0/8     0.0     3.4      2765/131128     2.1    Yes (free)             \n",
      "rich133-s30-10     0/12    0.0     0.9      1440/131128     1.1    Yes (free)             \n",
      "rich133-s30-11     0/12    0.0     2.1      1476/131128     1.1    Yes (free)             \n",
      "rich133-s30-12     0/12    0.0     1.8      1375/131128     1.0    Yes (free)             \n",
      "rich133-s30-13     0/12    0.0     0.1      1454/131128     1.1    Yes (free)             \n",
      "rich133-s30-14     0/12    0.0     0.7      1506/131128     1.1    Yes (free)             \n",
      "rich133-s30-15     0/12    0.0     1.3      1462/131128     1.1    Yes (free)             \n",
      "rich133-s30-16     0/12    0.0     0.0      1467/131128     1.1    Yes (free)             \n",
      "rich133-s30-17     0/12    0.0     0.0      1457/131128     1.1    Yes (free)             \n",
      "rich133-s30-18     0/12    0.0     1.3      1461/131128     1.1    Yes (free)             \n",
      "rich133-s30-19     0/12    0.0     0.2      1415/131128     1.1    Yes (free)             \n",
      "rich133-s30-20     0/24    0.0     0.1      1276/131127     1.0    Yes (free)             \n",
      "rich133-s30-21     0/12    0.0     0.0      1666/131128     1.3    Yes (free)             \n",
      "rich133-s30-22     0/12    0.0     0.5      1386/131128     1.1    Yes (free)             \n",
      "rich133-s42-21     0/8     0.0     0.8      1482/131128     1.1    Yes (free)             \n",
      "rich133-s42-22     0/8     0.0     6.4      1371/131128     1.0    Yes (free)             \n",
      "rich133-s42-23     0/8     0.0     5.9      1384/131128     1.1    Yes (free)             \n",
      "rich133-s42-24     0/8     0.0     0.4      1633/131128     1.2    Yes (free)             \n",
      "rich133-s42-25     0/8     0.0     3.0      1354/131128     1.0    Yes (free)             \n",
      "rich133-s42-26     0/8     0.0     0.2      1372/131128     1.0    Yes (free)             \n",
      "rich133-s42-27     0/8     0.0     0.8      1624/131128     1.2    Yes (free)             \n",
      "rich133-s42-28     0/8     0.0     0.0      1374/131128     1.0    Yes (free)             \n"
     ]
    }
   ],
   "source": [
    "pace-check-queue coc-ice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it out: open up this notebook on a head node and compare the list you get to the [orientation slides](http://pace.gatech.edu/sites/default/files/pace-ice_orientation_0.pdf).  You'll see that it has grown, and they haven't updated the orientation slides.  We'll just have to find out what all these new nodes are for ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### A word on running jupyter on pace-ice:\n",
    "\n",
    "As you've probably noticed, the screen refresh can be a bit laggy.  I'm looking for a solution for that.  In the mean time, know that you don't have to work directly in the notebook: you can work on you answers in the terminal, and then paste them into the notebook, as long as you're confident that they are correct.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Head node exercise 2 (1 pt):** For the next questions, I need you to log in to compute nodes to find out about them, but you need to be able to specify which type of compute nodes you are accessing.\n",
    "\n",
    "For each of the types of nodes that you see in the list of resources you've created, give me the `qsub` command to start an interactive job on that type of node, with the following requirements:\n",
    "\n",
    "* The job should give you exclusive access to one node and all its cores and devices.\n",
    "* The job should let you pop open an X window (like a notebook) if you want to.\n",
    "* The job should begin in the CSE6230 directory.\n",
    "* The job should end after 30 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qsub:\tstandard input and output must be a terminal for \n",
      "\tinteractive job submission\n"
     ]
    }
   ],
   "source": [
    "qsub -I -X -q coc-ice -l nodes=1:ppn=4:gpus=1,walltime=00:30:00 -d $CSE6230_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What have we got to work with?\n",
    "\n",
    "Now, we need to switch from a notebook running on the head node to one running on a compue node, so `File->Save and Checkpoint` this notebook and `File->Close and Halt` it.  (Now would also be a good time to `git add` and `git commit` changes to this file.)  Use one of your ineractive job scripts to connect to a compute node and run the notebook there.\n",
    "See you on the other side!\n",
    "\n",
    "---\n",
    "\n",
    "Okay, you're running on the compute node.\n",
    "\n",
    "**Compute node exercise 1 (1 pt):** Using bash scripting (`awk`, `grep`, `sed`) or any other tool you like (you could, e.g., write a python script in a separate file and call it, as long as you `git add` it), set the following variables so that the printout that follows is correct.  You script should be correct on any type of compute node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "CPU_NAME=`cat /proc/cpuinfo | grep -oP 'model name\\s:\\s\\K.*' | head -1`\n",
    "CORE_COUNT=`cat /proc/cpuinfo | grep 'model name' | wc -l`\n",
    "GPU_NAME=`nvidia-smi --query-gpu=gpu_name --format=csv | grep -v name| uniq`\n",
    "GPU_COUNT=`nvidia-smi --query-gpu=gpu_name --format=csv | grep -v name| wc -l`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This nodes has  cores: its architecture is (Manufacturer, Product Id) \n",
      "This node has no GPUs\n"
     ]
    }
   ],
   "source": [
    "echo \"This nodes has ${CORE_COUNT} cores: its architecture is (Manufacturer, Product Id) ${CPU_NAME}\"\n",
    "if [[ ! $GPU_COUNT || $GPU_COUNT == 0 ]] ;  then\n",
    "    echo \"This node has no GPUs\"\n",
    "else\n",
    "    echo \"This node has ${GPU_COUNT} GPUs: its/their architecture is (Manufacturer, Product Id) ${GPU_NAME}\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute node exercise 2 (1 pt):** After you have logged out of the compute node, use whatever resources published on the web you can find to estimate the peak single precision flop/s of this node (you only need to do this step for one of the types of nodes, not all of them)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flop/s fever\n",
    "\n",
    "We've got to scratch that itch: we just want to go fast.  Okay, let's get it out of our system, and we'll look at more practical computations in future assignments.\n",
    "\n",
    "You should choose one of the node types for this task.  Because this is more complex if multiple devices are involved\n",
    "**1 bonus point** is earned for choosing a node with GPUs.\n",
    "\n",
    "**Compute node exercise 3 (2 pts):** The command below will compile and runs essentially the following computation:\n",
    "\n",
    "```C\n",
    "for (i = 0; i < N; i++) {\n",
    "  for (j = 0; j < T; j++) {\n",
    "    a[i] = a[i] * b + c;\n",
    "  }\n",
    "}\n",
    "```\n",
    "And it will report the flop/s for the whole calculation.\n",
    "\n",
    "Except that the array `a` will be spread out: `Nh` entries will be on the host and `Nd` entries will be on each of the devices.  Try to find values of `Nh`, `Nd`, and `T`, and (optionally) compiler optimization flags that give you the highest flop/s.  Things to consider:\n",
    "\n",
    "- Try to make your whole computation run for about a second.\n",
    "- The time reported is the maximum time for any device: if one sits idle while the other finishes, it will rob you of flop/s.\n",
    "- I suggest looking at one type of device at a time: set one of `Nh` or `Nd` to zero.  Once you've found your best flop/s for that device, optimize the other, and then try to strike a balance.\n",
    "- Experiment with the merits of putting more weight on `Nh` and `Nd` vs more weight on `T`.\n",
    "- You can also choose to pass the option `Bs=X` to control the thread block size for the GPU, where `X` is a power of 2 between 64 and 2048."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OMP_PROC_BIND=spread OMP_NUM_THREADS=1  ./fma_prof 8 16384 64 -1 10000000 0.5 3.0\n",
      "[./fma_prof] Nh = 8, Nd = 16384, T = 10000000, block size = 64\n",
      "[./fma_prof]: 1.260250e-01 elapsed seconds\n",
      "[./fma_prof]: 655520000000 flops executed\n",
      "[./fma_prof]: 5.201509e+12 flop/s\n"
     ]
    }
   ],
   "source": [
    "make run_fma_prof Nh=8 Nd=16384 T=10000000 Bs=64 COPTFLAGS='-O -xHost' CUOPTFLAGS='-O'\n",
    " # modify this for peak flop/s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute Node Exercise 4 (2 pts):** Now let's see if we can make any transformations to the code to make a difference.\n",
    "\n",
    "We will run the same program, but with fused multiply add loops that you have tried to optimize.  You should edit the files\n",
    "`fma_loop_host_opt.cu` and/or `fma_loop_dev_opt.c`: they start out exactly the same as the reference implementations used above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21,25c21,25\n",
      "<   for (int i = 0; i < N; i++) {\n",
      "<     for (int j = 0; j < T; j++) {\n",
      "<       a[i] = a[i] * b + c;\n",
      "<     }\n",
      "<   }\n",
      "---\n",
      "> \tfor (int j = 0; j < T; j++) {\n",
      "> \t\tfor (int i = 0; i < N; i++) {\n",
      "> \t\t\ta[i] = a[i] * b + c;\n",
      "> \t\t}\n",
      "> \t}\n"
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "diff fma_loop_host.c fma_loop_host_opt.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff fma_loop_dev.cu fma_loop_dev_opt.cu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See if you can exploit vectorization, instruction level parallelism, and/or loop transformations to get a boost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OMP_PROC_BIND=spread OMP_NUM_THREADS=1  ./fma_prof_opt 8 16384 64 -1 10000000 0.5 3.0\n",
      "[./fma_prof_opt] Nh = 8, Nd = 16384, T = 10000000, block size = 64\n",
      "[./fma_prof_opt]: 7.668400e-02 elapsed seconds\n",
      "[./fma_prof_opt]: 655520000000 flops executed\n",
      "[./fma_prof_opt]: 8.548328e+12 flop/s\n"
     ]
    }
   ],
   "source": [
    "make run_fma_prof_opt Nh=8 Nd=16384 T=10000000 Bs=64 COPTFLAGS='-O -xHost' CUOPTFLAGS='-O'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting this work\n",
    "\n",
    "**Workstation exercise 1 (1 pt):** When you have completed the rest of this assignment, `git add` the changes to this file, the source files you modified, and any scripts you added, and `git commit` them.\n",
    "\n",
    "Assignments in this class are submitted by [transfering a copy][1] or your repository to the TA.\n",
    "\n",
    "You do this by:\n",
    "\n",
    "1. Creating a duplicate repository on github with a very specific name.  The name should be `cse6230-YYY-gtusername`, where `YYY` is a short name for the assignment.  In this case, `YYY` is `a1`.  Make it a private repository.\n",
    "\n",
    "2. Push the version of your work that you want to be graded to the `master` branch of that repository.\n",
    "\n",
    "3. Request a transfer of ownership to the organization `cse6230`, team `Admin`.\n",
    "\n",
    "If that seems like a lot to remember, I've made a script that does it for you.  If you have `GTUSER` defined as your userid in your environment, you can run the following script from the top directory:\n",
    "\n",
    "```\n",
    "./utils/submit_assignment.sh master a1\n",
    "```\n",
    "\n",
    "This means that you intend to submit your `master` branch for assignment `a1`.  If you did your development for this assignment on a different branch, say `assignment-1`, you would run\n",
    "\n",
    "```\n",
    "./utils/submit_assignment.sh assignment-1 a1\n",
    "```\n",
    "\n",
    "[1]: https://help.github.com/articles/transferring-a-repository-owned-by-your-personal-account/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
