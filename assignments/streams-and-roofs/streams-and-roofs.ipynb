{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streams and Roofs\n",
    "\n",
    "In this week's assignment we are going to make some roofline diagrams for some $n$-body problems.\n",
    "\n",
    "This week's assignment is meant to be run on a node with a Tesla P100 GPU.  Let's load in our class module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "module use $CSE6230_DIR/modulefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " =========================================================================\n",
      "|                                                                         |\n",
      "|       A note about python/3.6:                                          |\n",
      "|       PACE is lacking the staff to install all of the python 3          |\n",
      "|       modules, but we do maintain an anaconda distribution for          |\n",
      "|       both python 2 and python 3. As conda significantly reduces        |\n",
      "|       the overhead with package management, we would much prefer        |\n",
      "|       to maintain python 3 through anaconda.                            |\n",
      "|                                                                         |\n",
      "|       All pace installed modules are visible via the module avail       |\n",
      "|       command.                                                          |\n",
      "|                                                                         |\n",
      " =========================================================================\n"
     ]
    }
   ],
   "source": [
    "module load cse6230"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently Loaded Modulefiles:\n",
      "  1) curl/7.42.1\n",
      "  2) git/2.13.4\n",
      "  3) python/3.6\n",
      "  4) /nv/coc-ice/tisaac3/opt/pace-ice/modulefiles/jupyter/1.0\n",
      "  5) intel/16.0\n",
      "  6) cuda/8.0.44\n",
      "  7) cse6230/default\n"
     ]
    }
   ],
   "source": [
    "module list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And verify that we're running where we expect to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Sep  6 21:24:18 2018       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 367.55                 Driver Version: 367.55                    |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  On   | 0000:81:00.0     Off |                    0 |\n",
      "| N/A   24C    P0    25W / 250W |      0MiB / 16276MiB |      0%   E. Process |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID  Type  Process name                               Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!\n",
    "\n",
    "Now, about the $n$-body simulations we're going to run: a classical $n$-body simulation has each body, or *particle*, interacting with each other, for $n(n+1)/2$ total interactions.  That hardly matches up to the streaming kernels we've been talking about!  So we're going to simplify a bit.\n",
    "\n",
    "We are going to simulate $n$ infinitesimal particles circling around an infinitely massive sun at the origin.  In this system, the sun is unmoved, and the particles are not affected by each other.\n",
    "\n",
    "We're going to normalize our coefficients and say that each particle is an ordinary differential equation with *six* components: three of position $X=(x, y, z)$ and three of velocity $U=(u, v, w)$.  The position, is changed by the velocity, of course, but the velocity changes under acceleration that depends on position:\n",
    "\n",
    "$$\\begin{aligned} \\dot{X} &= V \\\\ \\dot{V} &= - \\frac{X}{|X|^3}.\\end{aligned}$$\n",
    "\n",
    "To discretize this differential equation, we are going to use a time stepping method called the Verlet leap-frog method, which is good for calculating long simulations of stable orbits.  Given a time step length `dt`, our pseudocode for one time step for one particle looks like the following:\n",
    "\n",
    "1. `X += 0.5 * dt * V`\n",
    "2. `R2 = X . X` (dot product)\n",
    "3. `R = sqrt (R2)`\n",
    "4. `IR3 = 1. / (R2 * R)`\n",
    "5. `V -= X * dt * IR3`\n",
    "6. `X += 0.5 * dt * V`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.** Assuming `sqrt` and `div` count for one flop each, and assuming `x, y, z` and `u, v, w` are **double-precision** floating point\n",
    "numbers, **estimate the arithmetic intensity of a *particle time step***.  You should ignore the time it takes to load `dt`.  Your answer should have units of flops / byte.  Give your answer in a new cell below this one, and show how you arrived at that number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo 0.302 flops/byte\n",
    "#the total flops of a particle time step: 1+3+3 + 5 + 1 + 2 + 1+3+3 + 1+3+3 = 29 flops\n",
    "#the amount of memory access: 3*8*4 = 96 bytes\n",
    "# Estimated arithmetic intensity is 29 / 96 = 0.302\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.** Using the peak theoretical **double-precision** flop/s of this node (flop/s on the CPUs and GPU combined), calculated the same way as in the last assignment, and reported peak memory bandwidths from the manufacturers, **estimate the system balance of CPUs and the GPU of this node separately**.  Note that the bandwidth estimate from intel will be for one socket (4 cores) with attached memory, and our node has two such sockets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 node 8 cores(2 sockets), 1 gpu  teslap100\n",
    "#from last assignment's calculation method, the peak flop/s is as below\n",
    "#CPU peak double-precision flops/s: 1.4 e+10 flop/s\n",
    "#GPU peak double-precision flop/s: 4.3 e+12 flop/s\n",
    "#from manufacturers reports, the bandwiths is as below:\n",
    "#CPU Intel® Xeon® Processor E5-2623 v4 68.3GB/s 6.83 e+10 byte/s\n",
    "#GPU Tesla P100 16GB 732GB/s 7.32 e+11 byte/s\n",
    "#So we have the machine ballance:\n",
    "#CPU machine ballance: 0.205\n",
    "#GPU machine ballance: 5.874"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last week, we didn't take the peak flop/s values from the manufacturers at face value, and this week we are not going to take the beak Gbyte/s for granted either.  Last week we used a custom benchmark in our calculations; this week we will use an industry standard: the\n",
    "[STREAM benchmark](https://www.cs.virginia.edu/stream/ref.html).\n",
    "\n",
    "We can run the stream benchmark on the CPUs for this assignment with a makefile target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "icc -g -Wall -fPIC -Ofast -I/usr/local/pacerepov1/cuda/8.0.44/include -qopenmp -o stream stream.c -DSTREAM_ARRAY_SIZE=40000000\n",
      "OMP_NUM_THREADS=1  ./stream\n",
      "-------------------------------------------------------------\n",
      "STREAM version $Revision: 5.10 $\n",
      "-------------------------------------------------------------\n",
      "This system uses 8 bytes per array element.\n",
      "-------------------------------------------------------------\n",
      "Array size = 40000000 (elements), Offset = 0 (elements)\n",
      "Memory per array = 305.2 MiB (= 0.3 GiB).\n",
      "Total memory required = 915.5 MiB (= 0.9 GiB).\n",
      "Each kernel will be executed 10 times.\n",
      " The *best* time for each kernel (excluding the first iteration)\n",
      " will be used to compute the reported bandwidth.\n",
      "-------------------------------------------------------------\n",
      "Number of Threads requested = 1\n",
      "Number of Threads counted = 1\n",
      "-------------------------------------------------------------\n",
      "Your clock granularity/precision appears to be 1 microseconds.\n",
      "Each test below will take on the order of 37167 microseconds.\n",
      "   (= 37167 clock ticks)\n",
      "Increase the size of the arrays if this shows that\n",
      "you are not getting at least 20 clock ticks per test.\n",
      "-------------------------------------------------------------\n",
      "WARNING -- The above is only a rough guideline.\n",
      "For best results, please be sure you know the\n",
      "precision of your system timer.\n",
      "-------------------------------------------------------------\n",
      "Function    Best Rate MB/s  Avg time     Min time     Max time\n",
      "Copy:           19610.9     0.032673     0.032635     0.032725\n",
      "Scale:          21165.5     0.030263     0.030238     0.030321\n",
      "Add:            19378.7     0.049875     0.049539     0.052205\n",
      "Triad:          19548.0     0.049152     0.049110     0.049179\n",
      "-------------------------------------------------------------\n",
      "Solution Validates: avg error less than 1.000000e-13 on all three arrays\n",
      "-------------------------------------------------------------\n",
      "-------------------------------------------------------------\n",
      "STREAM version $Revision: 5.10 $\n",
      "-------------------------------------------------------------\n",
      "This system uses 8 bytes per array element.\n",
      "-------------------------------------------------------------\n",
      "Array size = 40000000 (elements), Offset = 0 (elements)\n",
      "Memory per array = 305.2 MiB (= 0.3 GiB).\n",
      "Total memory required = 915.5 MiB (= 0.9 GiB).\n",
      "Each kernel will be executed 10 times.\n",
      " The *best* time for each kernel (excluding the first iteration)\n",
      " will be used to compute the reported bandwidth.\n",
      "-------------------------------------------------------------\n",
      "Number of Threads requested = 8\n",
      "Number of Threads counted = 8\n",
      "-------------------------------------------------------------\n",
      "Your clock granularity/precision appears to be 1 microseconds.\n",
      "Each test below will take on the order of 37251 microseconds.\n",
      "   (= 37251 clock ticks)\n",
      "Increase the size of the arrays if this shows that\n",
      "you are not getting at least 20 clock ticks per test.\n",
      "-------------------------------------------------------------\n",
      "WARNING -- The above is only a rough guideline.\n",
      "For best results, please be sure you know the\n",
      "precision of your system timer.\n",
      "-------------------------------------------------------------\n",
      "Function    Best Rate MB/s  Avg time     Min time     Max time\n",
      "Copy:           19579.7     0.032726     0.032687     0.032759\n",
      "Scale:          21056.1     0.030640     0.030395     0.032480\n",
      "Add:            19274.7     0.049891     0.049806     0.050129\n",
      "Triad:          19443.5     0.049405     0.049374     0.049466\n",
      "-------------------------------------------------------------\n",
      "Solution Validates: avg error less than 1.000000e-13 on all three arrays\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "make runstream STREAM_N=40000000 COPTFLAGS=-Ofast OMP_NUM_THREADS=8 OMP_PROC_BIND=spread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `STREAM_N` argument will control the size of the stream arrays.\n",
    "\n",
    "**Question 3:** Modify the invocation of `make runstreams` by modifying the values of\n",
    "`STREAM_N`, `COPTFLAGS` (optimization flags), and/or `OMPENV` (the openMP environment variables) to get the largest streaming bandwidth from main memory that you can for this node.\n",
    "\n",
    "- Follow the directions in the output of the file and make sure you are testing streaming bandwidth from memory and not from a higher level of cache.\n",
    "- You should try to get close to the same bandwidth for all tests:\n",
    "\n",
    "- There are two variables in the openMP environment you should care about, OMP_NUM_THREADS, which is self explanatory, and OMP_PROC_BIND is discussed [here](http://pages.tacc.utexas.edu/~eijkhout/pcse/html/omp-affinity.html).  **You should try to use as few threads as possible** to achieve peak bandwidth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4:** What does `OMP_PROC_BIND=close` mean, and why is it a bad choice, not just for this benchmark, but for any streaming kernel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#it is a bad choice because when using OMP_PROC_BIND=close, the assignents \n",
    "#goes successively through available places, where they could ends up being on\n",
    "#the same socket, thus reducing the bandwith."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5:** I've modified the benchmark, calling it `stream2.c`.  Here's the difference, it's one line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267d266\n",
      "< #pragma omp parallel for\n"
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "diff stream.c stream2.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy your options for `runstream` to `runstream2` below.  The reported results should be different: why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The result is different because in stream2.c, we didn't do the parallelization\n",
    "#for the initialization of the arrays, and this causes that all memories will be\n",
    "#associated with the master thread's socket. Which makes it significantly slower\n",
    "#to run because subsequent acess by other sockets will access data from remote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "icc -g -Wall -fPIC -Ofast -I/usr/local/pacerepov1/cuda/8.0.44/include -qopenmp -o stream2 stream2.c -DSTREAM_ARRAY_SIZE=40000000\n",
      "OMP_NUM_THREADS=1  ./stream2\n",
      "-------------------------------------------------------------\n",
      "STREAM version $Revision: 5.10 $\n",
      "-------------------------------------------------------------\n",
      "This system uses 8 bytes per array element.\n",
      "-------------------------------------------------------------\n",
      "Array size = 40000000 (elements), Offset = 0 (elements)\n",
      "Memory per array = 305.2 MiB (= 0.3 GiB).\n",
      "Total memory required = 915.5 MiB (= 0.9 GiB).\n",
      "Each kernel will be executed 10 times.\n",
      " The *best* time for each kernel (excluding the first iteration)\n",
      " will be used to compute the reported bandwidth.\n",
      "-------------------------------------------------------------\n",
      "Number of Threads requested = 1\n",
      "Number of Threads counted = 1\n",
      "-------------------------------------------------------------\n",
      "Your clock granularity/precision appears to be 1 microseconds.\n",
      "Each test below will take on the order of 37244 microseconds.\n",
      "   (= 37244 clock ticks)\n",
      "Increase the size of the arrays if this shows that\n",
      "you are not getting at least 20 clock ticks per test.\n",
      "-------------------------------------------------------------\n",
      "WARNING -- The above is only a rough guideline.\n",
      "For best results, please be sure you know the\n",
      "precision of your system timer.\n",
      "-------------------------------------------------------------\n",
      "Function    Best Rate MB/s  Avg time     Min time     Max time\n",
      "Copy:           21181.5     0.030305     0.030215     0.030797\n",
      "Scale:          20546.5     0.031238     0.031149     0.031818\n",
      "Add:            19536.8     0.049199     0.049138     0.049270\n",
      "Triad:          19347.1     0.049976     0.049620     0.052317\n",
      "-------------------------------------------------------------\n",
      "Solution Validates: avg error less than 1.000000e-13 on all three arrays\n",
      "-------------------------------------------------------------\n",
      "-------------------------------------------------------------\n",
      "STREAM version $Revision: 5.10 $\n",
      "-------------------------------------------------------------\n",
      "This system uses 8 bytes per array element.\n",
      "-------------------------------------------------------------\n",
      "Array size = 40000000 (elements), Offset = 0 (elements)\n",
      "Memory per array = 305.2 MiB (= 0.3 GiB).\n",
      "Total memory required = 915.5 MiB (= 0.9 GiB).\n",
      "Each kernel will be executed 10 times.\n",
      " The *best* time for each kernel (excluding the first iteration)\n",
      " will be used to compute the reported bandwidth.\n",
      "-------------------------------------------------------------\n",
      "Number of Threads requested = 8\n",
      "Number of Threads counted = 8\n",
      "-------------------------------------------------------------\n",
      "Your clock granularity/precision appears to be 1 microseconds.\n",
      "Each test below will take on the order of 37228 microseconds.\n",
      "   (= 37228 clock ticks)\n",
      "Increase the size of the arrays if this shows that\n",
      "you are not getting at least 20 clock ticks per test.\n",
      "-------------------------------------------------------------\n",
      "WARNING -- The above is only a rough guideline.\n",
      "For best results, please be sure you know the\n",
      "precision of your system timer.\n",
      "-------------------------------------------------------------\n",
      "Function    Best Rate MB/s  Avg time     Min time     Max time\n",
      "Copy:           21116.5     0.030337     0.030308     0.030373\n",
      "Scale:          20387.5     0.031413     0.031392     0.031447\n",
      "Add:            19480.2     0.049324     0.049281     0.049366\n",
      "Triad:          19281.8     0.049860     0.049788     0.050062\n",
      "-------------------------------------------------------------\n",
      "Solution Validates: avg error less than 1.000000e-13 on all three arrays\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "make runstream2 STREAM_N=40000000 COPTFLAGS=-Ofast OMP_NUM_THREADS=8 OMP_PROC_BIND=spread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6:** Now we're going to run stream benchmarks for the GPU.  As above, modify the array size until you believe you are testing streaming bandwidth from memory and not from cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make runstreamcu STREAM_N=40000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7 (2 pts):** This is final time we're running a stream benchmark, I promise.  This benchmark is also for the GPU, but instead of the arrays originating in the GPUs memory, they start on the CPUs memory, and must be transfered to the GPU in back.  This mimics a common design pattern when people try to modify their code for GPUs: identify the bottleneck kernel, and try to \"offload\" it to the GPU, where it will have a higher throughput (once it get's there).  You don't have to modify this run, I just want you to see what bandwidths it reports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc -ccbin=icpc -lineinfo -Xcompiler '-fPIC' -O -o streamcu2 stream2.cu -DSTREAM_ARRAY_SIZE=1000000\n",
      "nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "stream2.cu(569): warning: variable \"i\" was set but never used\n",
      "\n",
      "stream2.cu(569): warning: variable \"i\" was set but never used\n",
      "\n",
      "./streamcu2\n",
      "-------------------------------------------------------------\n",
      "CSE6230 CUDA STREAM based on version $Revision: 5.10 $\n",
      "-------------------------------------------------------------\n",
      "This system uses 8 bytes per array element.\n",
      "-------------------------------------------------------------\n",
      "Array size = 1000000 (elements), Offset = 0 (elements)\n",
      "Memory per array = 7.6 MiB (= 0.0 GiB).\n",
      "Total memory required = 22.9 MiB (= 0.0 GiB).\n",
      "Each kernel will be executed 10 times.\n",
      " The *best* time for each kernel (excluding the first iteration)\n",
      " will be used to compute the reported bandwidth.\n",
      "Ordinal of GPUs requested = 0\n",
      "  Device name: Tesla P100-PCIE-16GB\n",
      "  Memory Clock Rate (KHz): 715000\n",
      "  Memory Bus Width (bits): 4096\n",
      "  Peak Memory Bandwidth (GB/s): 732.160000\n",
      "\n",
      "-------------------------------------------------------------\n",
      "1.000000 2.000000 0.000000\n",
      "-------------------------------------------------------------\n",
      "Your clock granularity/precision appears to be 1 microseconds.\n",
      "Each test below will take on the order of 5661 microseconds.\n",
      "   (= 5661 clock ticks)\n",
      "Increase the size of the arrays if this shows that\n",
      "you are not getting at least 20 clock ticks per test.\n",
      "-------------------------------------------------------------\n",
      "WARNING -- The above is only a rough guideline.\n",
      "For best results, please be sure you know the\n",
      "precision of your system timer.\n",
      "-------------------------------------------------------------\n",
      "Function    Best Rate MB/s  Avg time     Min time     Max time\n",
      "Copy:            6159.0     0.002599     0.002598     0.002600\n",
      "Scale:           6154.0     0.002601     0.002600     0.002604\n",
      "Add:             6172.6     0.003891     0.003888     0.003892\n",
      "Triad:           6171.5     0.003891     0.003889     0.003894\n",
      "-------------------------------------------------------------\n",
      "Solution Validates: avg error less than 1.000000e-12 on all three arrays\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "make runstreamcu2 STREAM_N=1000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with the three peak bandwidths that we have *computed* (not the reported values from question 2) -- CPU, GPU with arrays on the GPU, and GPU with arrays on the CPU -- and with the theoretical peak flop/s for the CPU and GPU, compute *effective system balances* and create a plot with rooflines for all three balances overlayed.\n",
    "\n",
    "- The y axis should be absolute Gflop/s, not relative, so we can compare them, and should be labeled \"Gflop/s\"\n",
    "- Label with roofline goes with which balance: \"CPU\", \"GPU\", \"CPU->GPU->CPU\"\n",
    "- The x axies should be in units of \"double precision flops / byte\"\n",
    "\n",
    "Save your plot as the jpg `threerooflines.jpg` so that it can embed in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CPU Bandwidth: 65.632 GB/s Peak Performance: 140 Gflop/s\n",
    "#GPU Bandwidth: 509.043 GB/s Peak Performance: 4300 Gflop/s\n",
    "#CPU-GPU-CPU Bandwidth : 6.220 GB/s Peak Performance: 4320 Gflop/s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Three rooflines](./threerooflines.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8 (2 pts):** Remember those particles all the way back in question 1?  Your arithmetic intensity estimate could be placed on the roofline plot for the CPUs, and you could make a judgement about whether the kernel is compute bound or memory bound.\n",
    "\n",
    "Now let's put it to the test.  The `make runcloud` target simulates `NPOINT` particles orbiting the sun for `NT` time steps.  Because these particles are independent, you can optionally \"chunk\" multiple time steps for each particle independent of the other particles.  Doing this reduces the number of memory accesses per flop:  each particle stays in register for `NCHUNK` time steps.\n",
    "\n",
    "Do your best to optimize the throughput of the simulation both in the limit of few particles and many time steps, and in the limit of many particles and few time steps.\n",
    "Do that by modifying the commands below.\n",
    "\n",
    "- Make the simulations each run about a second\n",
    "- Do your best to optimize the compiler flags and the runtime (openMP) environment\n",
    "\n",
    "Using the outputs of those runs, estimate the *effective* arithmetic intensity: take the peak flop/s of the CPU and divide by the throughput of particle time steps per second.  Give that effective arithmetic intensity below.  If that differs from the estimated arithmetic intensity from the first question, can you point to any assumptions that we made that are bad and could explain the difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f cloud cloud.o verlet.o\n",
      "make verlet.o DEFINES=\"-DNT=1\"\n",
      "make[1]: Entering directory `/nv/coc-ice/yzhao492/cse6230/assignments/streams-and-roofs'\n",
      "icc -std=c99 -g -Wall -fPIC -O -qopt-report=3 -I/usr/local/pacerepov1/cuda/8.0.44/include -DNT=1 -qopenmp -c -o verlet.o verlet.c\n",
      "icc: remark #10397: optimization reports are generated in *.optrpt files in the output location\n",
      "make[1]: Leaving directory `/nv/coc-ice/yzhao492/cse6230/assignments/streams-and-roofs'\n",
      "make cloud\n",
      "make[1]: Entering directory `/nv/coc-ice/yzhao492/cse6230/assignments/streams-and-roofs'\n",
      "icc -std=c99 -g -Wall -fPIC -O -qopt-report=3 -I/usr/local/pacerepov1/cuda/8.0.44/include  -qopenmp -c -o cloud.o cloud.c\n",
      "icc: remark #10397: optimization reports are generated in *.optrpt files in the output location\n",
      "icpc -qopenmp -o cloud verlet.o cloud.o -Wl,-rpath,.\n",
      "make[1]: Leaving directory `/nv/coc-ice/yzhao492/cse6230/assignments/streams-and-roofs'\n",
      "OMP_NUM_THREADS=1  ./cloud 32 1000000 0.01 1\n",
      "./cloud, NUM_POINTS=32, NUM_STEPS=1000000, DT=0.01, NCHUNK=1\n",
      "[./cloud]: 2.212880e-01 elapsed seconds\n",
      "[./cloud]: 1.446080e+08 particle time steps per second\n",
      "[./cloud]: 1.446080e+08 particle time step chunks per second\n"
     ]
    }
   ],
   "source": [
    "make runcloud NPOINT=32 NT=10000000 NCHUNK=128 COPTFLAG=-O3 OMP_NUM_THREAD=8 OMP_PROC_BIND=spread\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make runcloud NPOINT=6400000 NT=100 NCHUNK=1 COPTFLAG=-O3 OMP_NUM_THREAD=8 OMP_PROC_BIND=spread\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#few particles, many time steps: 3.22e+08 particle time steps per second\n",
    "# many particles, few time steps: 7.25e+08 particle time steps per second\n",
    "#effective arithmetic intensity:\n",
    "# 6.56e+10 flops/s * 1s / (7.25e+8 * 96 bytes) = 0.94 flops/byte\n",
    "#it differs from our calculation in the beggining, which is 0.302\n",
    "#One explanation is that the assumption we made, that the sqrt and / all counts\n",
    "# as 1 flop, is wrong, because, you know, sqrt takes about 20 flops and divide \n",
    "#takes about 15 flops, that will gives us around 60 flop per particle time steps,\n",
    "#which makes the arithmetic intensity 0.67, which is closer to our observation.\n",
    "#Another issue might be that the memory is occupied by some other programs, and \n",
    "#thus it is not fully engaged in our calculation, which also increases the effective\n",
    "#arithmetic intensity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
