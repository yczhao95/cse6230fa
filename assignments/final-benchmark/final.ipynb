{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "\n",
    "## Due Thursday, December 13th at 5:00 PM EDT, no extensions\n",
    "\n",
    "Those working in pairs should submit the same version of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1: Successfully install and run your benchmark (3 pts)**\n",
    "\n",
    "Include in this directory an example **job submission script** that runs your benchmark code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "To run the benchmark, got to the root directory and run\n",
    "python runbenchmark.py\n",
    "To run the benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2: Develop a performance model for your benchmark (5 pts)**\n",
    "\n",
    "In the last assignment, you chose a performance metric of your benchmark, let's call it $V$.  Your benchmark will solve a problem with some parameters (problem size, the choice of matrix / network / etc.), let's call those parameters $N$.  The node that you chose to run on will have some machine parameters (The number of cores, the type of GPU, the bandwidth from main memory, etc., etc.), let's call them $P$.  Give an expression \n",
    "for $V(N,P)$ for your benchmark, and describe how you arrived at it.  You should use your discretion when choosing the level of detail.  If it is hard to develop a closed-form performance model for the whole benchmark, but there are a few key kernels that happens repeatedly in your benchmark (a stencil application, an iteration of stochastic gradient descent, etc.), you can give performance models for those benchmarks(s) instead.\n",
    "\n",
    "If it is difficult to formulate your expression in terms of machine parameters, try to develop an expression\n",
    "with coefficients that measure the rates at which the machine can do some lower-level operations (for example, the rate at which a GPU can sum an array).  If you have these coefficients, you should give a plausible description for how the architecture of the machine affects those rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The Graph 500 Benchmark consists of 3 kernals:\n",
    "    Kernal 1 constructs an undirected graph (accessable for the rest two kernals) from an edge list;\n",
    "    Kernal 2 conducts a sinlge bread first search on the graph;\n",
    "    Kernal 3 runs mutiple single-source shortest path computations on the graph.\n",
    "The Scaling parameter mainly consists of two free parameters: N: the logarithm base two of the number of vertices in \n",
    "the graph; and M: number of edges. When run, the input is as scale = N,  and edgefator = M / N;\n",
    "\n",
    "As for the value of the benchmark, multiple quantities are being reported.\n",
    "The main performance metric we are looking at is the traversed edges per second(TEPS), which is measured throught \n",
    "kernal 2 and kernal 3. E.g., suppose the the time to run kernal 2 with is t2(N), then TEPS_kernal2(N) = M / t2(N). \n",
    "The benchmark also reports the construction time for kernal1, the harmonic_mean_TEPS and harmonic_stddev_TEPS for\n",
    "DFS and SSSP (since TEPS is rate, they are compared using harmonic means). Besides, the min_TEPS, firstquartile_TEPS,\n",
    "median TEPS, thirdquartile_TEPS, max_TEPS is also reported for both DFS and SSSP.\n",
    "\n",
    "So the V is TEPS, and N would be (Nv, edgefactor). \n",
    "The P is difficult to formulate, for graph 500, the main machine factor that affects the TEPS would be the whole system\n",
    "memory bandwidth and memory latency, since the graph algorithm has a very low time-space locality, and it is intended \n",
    "for big data anaylyisis and similar dense data senarios.\n",
    "Let us suppose that we have:\n",
    "    memory bandwidth MB\n",
    "    memory latency ML\n",
    "Then, TEPS = c * MB / (ML * Log(NT * edgefactor)) where c would be an coeeficient.\n",
    "As we can see, with a higher bandwidth and lower latency from the machine will increase the peak\n",
    "TEPS that this bencmark can reach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Gather statistics for the performance metric (3 pts)**\n",
    "\n",
    "Include in this directory the **job script(s)** that you use to gather statistics for the performance metric on pace-ice.  Additionally, describe what steps you've taken to ensure the quality of the statistics: how are you accounting for variability / noise?  Does your benchmark show different performance on the first run than on subsequent runs?\n",
    "\n",
    "If you are running your benchmark for multiple problem instances ($N$), include a plot of the performance metric for the different problem instances. (You can include error bars for maximum/minimum values of the performance metric for the same problem instance to convey variability.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Compare your performance model to your statistics (5 pts)**\n",
    "\n",
    "If your performance model allowed you to make a concrete prediction of $V(N,P)$ before running the benchmark,\n",
    "compare (in a table or plot) the predictions and the actual measurements.\n",
    "\n",
    "If your performance model includes coefficients that could not be estimated ahead of time, use measurements gathered during your experiments to get empirical values of those coefficients to fit the model to the data.  Once you do this, answer the following question: is there a plausible explanation (in terms of the architecture of the machine and the nature of the algorithm) for why these coefficients have the value that they do?\n",
    "\n",
    "Present additional timings and/or machine performance metrics (either for the full benchmark or key kernels) and make a case for why you think they best demonstrate what the biggest bottleneck to the performance of the benchmark is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. Elaborate on the benchmark (4 pts)**\n",
    "\n",
    "Choose one of the following.  **2 bonus points** if your discussion is supported by citations (whose meaning you summarize in your discussion)\n",
    "\n",
    "- **Change the machine:** using machine characteristics from other current or from speculative architectures, describe a different type of node could get better performance on this benchmark, and predict the performance of the benchmark on that architecture.\n",
    "\n",
    "- **Change the algorithm/implementation:** predict the performance on the same node that you used, but with a different algorithm/implementation that solves the same problem as the benchmark code.\n",
    "\n",
    "- **Change the benchmark:** choose an application that solve a problem related to the problem solved in the benchmark.  If you do not expect the benchmark performance metric to be predictive for that application, explain why and try to change the benchmark to be more predictive, and predict what the performance of the new benchmark would be on the same node."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
