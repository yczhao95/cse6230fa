{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "\n",
    "## Due Thursday, December 13th at 5:00 PM EDT, no extensions\n",
    "\n",
    "Those working in pairs should submit the same version of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1: Successfully install and run your benchmark (3 pts)**\n",
    "\n",
    "Include in this directory an example **job submission script** that runs your benchmark code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2: Develop a performance model for your benchmark (5 pts)**\n",
    "\n",
    "In the last assignment, you chose a performance metric of your benchmark, let's call it $V$.  Your benchmark will solve a problem with some parameters (problem size, the choice of matrix / network / etc.), let's call those parameters $N$.  The node that you chose to run on will have some machine parameters (The number of cores, the type of GPU, the bandwidth from main memory, etc., etc.), let's call them $P$.  Give an expression \n",
    "for $V(N,P)$ for your benchmark, and describe how you arrived at it.  You should use your discretion when choosing the level of detail.  If it is hard to develop a closed-form performance model for the whole benchmark, but there are a few key kernels that happens repeatedly in your benchmark (a stencil application, an iteration of stochastic gradient descent, etc.), you can give performance models for those benchmarks(s) instead.\n",
    "\n",
    "If it is difficult to formulate your expression in terms of machine parameters, try to develop an expression\n",
    "with coefficients that measure the rates at which the machine can do some lower-level operations (for example, the rate at which a GPU can sum an array).  If you have these coefficients, you should give a plausible description for how the architecture of the machine affects those rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Gather statistics for the performance metric (3 pts)**\n",
    "\n",
    "Include in this directory the **job script(s)** that you use to gather statistics for the performance metric on pace-ice.  Additionally, describe what steps you've taken to ensure the quality of the statistics: how are you accounting for variability / noise?  Does your benchmark show different performance on the first run than on subsequent runs?\n",
    "\n",
    "If you are running your benchmark for multiple problem instances ($N$), include a plot of the performance metric for the different problem instances. (You can include error bars for maximum/minimum values of the performance metric for the same problem instance to convey variability.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Compare your performance model to your statistics (5 pts)**\n",
    "\n",
    "If your performance model allowed you to make a concrete prediction of $V(N,P)$ before running the benchmark,\n",
    "compare (in a table or plot) the predictions and the actual measurements.\n",
    "\n",
    "If your performance model includes coefficients that could not be estimated ahead of time, use measurements gathered during your experiments to get empirical values of those coefficients to fit the model to the data.  Once you do this, answer the following question: is there a plausible explanation (in terms of the architecture of the machine and the nature of the algorithm) for why these coefficients have the value that they do?\n",
    "\n",
    "Present additional timings and/or machine performance metrics (either for the full benchmark or key kernels) and make a case for why you think they best demonstrate what the biggest bottleneck to the performance of the benchmark is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. Elaborate on the benchmark (4 pts)**\n",
    "\n",
    "Choose one of the following.  **2 bonus points** if your discussion is supported by citations (whose meaning you summarize in your discussion)\n",
    "\n",
    "- **Change the machine:** using machine characteristics from other current or from speculative architectures, describe a different type of node could get better performance on this benchmark, and predict the performance of the benchmark on that architecture.\n",
    "\n",
    "- **Change the algorithm/implementation:** predict the performance on the same node that you used, but with a different algorithm/implementation that solves the same problem as the benchmark code.\n",
    "\n",
    "- **Change the benchmark:** choose an application that solve a problem related to the problem solved in the benchmark.  If you do not expect the benchmark performance metric to be predictive for that application, explain why and try to change the benchmark to be more predictive, and predict what the performance of the new benchmark would be on the same node."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
