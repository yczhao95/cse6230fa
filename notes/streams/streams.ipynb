{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cores & Memory: Streaming Kernels\n",
    "\n",
    "## - Assignment 1 Miscellany\n",
    "## - Review: Little's Law, CPUs vs. GPUs\n",
    "## - Streaming Kernels\n",
    "## - Arithmetic Intensity, Machine Balance, & the Roofline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## First-Week-Flops Miscellany\n",
    "\n",
    "### My mistakes:\n",
    "\n",
    "#### - It took a few tries to get the jupyter notebook job working, but it does now\n",
    "\n",
    "#### - `nvcc` syntax: use `-lineinfo` for symbols in binaries; `-G` turns off all optimizations :(\n",
    "\n",
    "#### - I'll try to include `module load cse6230` in future assignments.  (If it doesn't work, check if modules are loaded first)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Useful commands:\n",
    "\n",
    "#### - `pbsnodes -a` / `qnodes -a` on the head node\n",
    "\n",
    "#### - `nvidia-smi`: present on all nodes with GPUs.  Useful in and of itself, but also good for detecting when no GPUs are present: `which nvidia-smi || echo \"No GPUs\"`\n",
    "\n",
    "### Questions / confusion people have had:\n",
    "\n",
    "#### - \"Intel says this is a 6-core / 12-hardware thread package; PACE says this is a 12 core node.  Is PACE wrong?\"\n",
    "\n",
    "Try this on a compute node when you have X forwarding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module load hwloc\n",
    "lstopo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll see that PACE has it right because, for all of the nodes we will be using, they have installed *two sockets per node*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### There is now a grading script that you can try for yourself\n",
    "\n",
    "`./grading-script.sh` will:\n",
    "\n",
    "- split up your notebook into host and compute node components\n",
    "- use your qsub expressions to run your compute node script on one of each type of node\n",
    "- even though you tune for one type of node, it should run without crashing on any node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For those seeking peak GPU performance, you can also control the \"grid size\" (number of thread blocks)\n",
    "\n",
    "Add, e.g., `Gs=15` to the `run_fma_prof` and `run_fma_prof_opt` targets\n",
    "\n",
    "This will let you experiment with ILP vs. TLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Review: Little's Law\n",
    "\n",
    "$\\Huge L = \\lambda W$\n",
    "\n",
    "#### - $L$: amount of $x$ in a system.\n",
    "#### - $\\lambda$: arrival rate, $x$ / sec.\n",
    "#### - $W$: time spent in the system (sec).\n",
    "\n",
    "- Note that it has dimensions $x$, whatever it is we're trying to count.\n",
    "- For pipelines, we often think of $\\lambda$ as $x$ / cycle and $W$ as length of the pipeline in cycles.  Every unit but $x$ cancels out, so we can take whichever form is more convenient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example from last time: how many independent FMAs are needed for peak flop/s?\n",
    "\n",
    "#### - $W$: depth of the pipeline\n",
    "#### - $\\lambda$: arrival rate = # FPUs * # FMAs in a *vectorized* instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[[Intel's intrisics guide](https://software.intel.com/sites/landingpage/IntrinsicsGuide/)]\n",
    "\n",
    "![vfmadd132ps](./images/intel-intrinsics.png)\n",
    "\n",
    "latency = $W$, CPI (cycles per instruction) = 1 / $\\lambda_v$, where $\\lambda_v$ = throughput for *vectorized* FMAs.  Multiply by vector width (see operation pseudocode) to get full $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Question from last time:\n",
    "\n",
    "#### - What is the latency of FMA on the GPUs?  How could we estimate it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Quick Review: CPUs  (Hosts)\n",
    "\n",
    "#### - One set of instructions per thread\n",
    "#### - OS schedules threads (software multithreading), can *migrate* them between cores\n",
    "#### - x86-64 (AVX2) instruction set has 32 256-bit vector registers per threads\n",
    "#### - Parallelism via software multithread, hardware multithreading, superscalar execution, vectorization (**SIMD**)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Quick Review: GPUs (Devices)\n",
    "\n",
    "(See the nice illustrations from Prof. Vuduc's [slides](http://vuduc.org/cse6230/slides/cse6230-fa14--05-cuda.pdf), starting on slide 27)\n",
    "\n",
    "#### - A *compute kernel* is a task that the host assigns to the device in a kernel launch\n",
    "\n",
    "```c++\n",
    "\n",
    "solveForX<<<ThreadsPerBlock,BlocksPerGrid>>>(A,x,y);\n",
    "```\n",
    "\n",
    "- Proceeds asynchronously from the host until the host requires the results\n",
    "\n",
    "#### - The task is broken down into a **grid** of *independent* thread blocks\n",
    "\n",
    "- The host has no control over which thread blocks are assigned where and in what order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### - Each thread block is assigned to a streaming multiprocessor (SM), where it stays\n",
    "\n",
    "- A SM may be assigned multiple thread blocks\n",
    "\n",
    "#### - The SM breaks down the thread blocks into **warps** (32 threads): a warp shares an instruction set, so all (non-divergent) instructions are vectorized\n",
    "#### - The SM issues instructions from multiple warps per cycle (sometimes multiple instructions per warp per cycle) \n",
    "#### - When a warp is stalled, another is scheduled\n",
    "#### - Avoid: thread divergence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Using `nvprof` to estimate the FMA latency\n",
    "\n",
    "`nvprof` is a performance analysis tool like `perf` and `gprof` combined for the GPU\n",
    "\n",
    "It can be inserted before a program in the same way as `perf`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make run_fma_prof Nh=0 Bs=1024 Gs=15 Nd=$((1024*15))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
