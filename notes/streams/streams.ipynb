{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cores & Memory: Streaming Kernels\n",
    "\n",
    "## - Assignment 1 Miscellany\n",
    "## - Review: Little's Law, CPUs vs. GPUs\n",
    "## - Streaming Kernels\n",
    "## - Arithmetic Intensity, Machine Balance, & the Roofline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## First-Week-Flops Miscellany\n",
    "\n",
    "### My mistakes:\n",
    "\n",
    "#### - It took a few tries to get the jupyter notebook job working, but it does now\n",
    "\n",
    "#### - `nvcc` syntax: use `-lineinfo` for symbols in binaries; `-G` turns off all optimizations :(\n",
    "\n",
    "#### - I'll try to include `module load cse6230` in future assignments.  (If it doesn't work, check if modules are loaded first)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Useful commands:\n",
    "\n",
    "#### - `pbsnodes -a` / `qnodes -a` on the head node\n",
    "\n",
    "#### - `nvidia-smi`: present on all nodes with GPUs.  Useful in and of itself, but also good for detecting when no GPUs are present: `which nvidia-smi || echo \"No GPUs\"`\n",
    "\n",
    "### Questions / confusion people have had:\n",
    "\n",
    "#### - \"Intel says this is a 6-core / 12-hardware thread package; PACE says this is a 12 core node.  Is PACE wrong?\"\n",
    "\n",
    "Try this on a compute node when you have X forwarding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine (128GB total)\n",
      "  NUMANode L#0 (P#0 64GB)\n",
      "    Socket L#0 + L3 L#0 (15MB)\n",
      "      L2 L#0 (256KB) + L1d L#0 (32KB) + L1i L#0 (32KB) + Core L#0 + PU L#0 (P#0)\n",
      "      L2 L#1 (256KB) + L1d L#1 (32KB) + L1i L#1 (32KB) + Core L#1 + PU L#1 (P#1)\n",
      "      L2 L#2 (256KB) + L1d L#2 (32KB) + L1i L#2 (32KB) + Core L#2 + PU L#2 (P#2)\n",
      "      L2 L#3 (256KB) + L1d L#3 (32KB) + L1i L#3 (32KB) + Core L#3 + PU L#3 (P#3)\n",
      "      L2 L#4 (256KB) + L1d L#4 (32KB) + L1i L#4 (32KB) + Core L#4 + PU L#4 (P#4)\n",
      "      L2 L#5 (256KB) + L1d L#5 (32KB) + L1i L#5 (32KB) + Core L#5 + PU L#5 (P#5)\n",
      "    HostBridge L#0\n",
      "      PCIBridge\n",
      "        PCI 15b3:1003\n",
      "          Net L#0 \"ib0\"\n",
      "          OpenFabrics L#1 \"mlx4_0\"\n",
      "      PCIBridge\n",
      "        PCI 10de:1023\n",
      "      PCIBridge\n",
      "        PCIBridge\n",
      "          PCI 1a03:2000\n",
      "      PCIBridge\n",
      "        PCI 8086:1521\n",
      "          Net L#2 \"eth0\"\n",
      "        PCI 8086:1521\n",
      "          Net L#3 \"eth1\"\n",
      "      PCI 8086:8d02\n",
      "        Block L#4 \"sda\"\n",
      "  NUMANode L#1 (P#1 64GB)\n",
      "    Socket L#1 + L3 L#1 (15MB)\n",
      "      L2 L#6 (256KB) + L1d L#6 (32KB) + L1i L#6 (32KB) + Core L#6 + PU L#6 (P#6)\n",
      "      L2 L#7 (256KB) + L1d L#7 (32KB) + L1i L#7 (32KB) + Core L#7 + PU L#7 (P#7)\n",
      "      L2 L#8 (256KB) + L1d L#8 (32KB) + L1i L#8 (32KB) + Core L#8 + PU L#8 (P#8)\n",
      "      L2 L#9 (256KB) + L1d L#9 (32KB) + L1i L#9 (32KB) + Core L#9 + PU L#9 (P#9)\n",
      "      L2 L#10 (256KB) + L1d L#10 (32KB) + L1i L#10 (32KB) + Core L#10 + PU L#10 (P#10)\n",
      "      L2 L#11 (256KB) + L1d L#11 (32KB) + L1i L#11 (32KB) + Core L#11 + PU L#11 (P#11)\n",
      "    HostBridge L#6\n",
      "      PCIBridge\n",
      "        PCI 10de:1023\n"
     ]
    }
   ],
   "source": [
    "module load hwloc\n",
    "lstopo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll see that PACE has it right because, for all of the nodes we will be using, they have installed *two sockets per node*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### There is now a grading script that you can try for yourself\n",
    "\n",
    "`./grading-script.sh` will:\n",
    "\n",
    "- split up your notebook into host and compute node components\n",
    "- use your qsub expressions to run your compute node script on one of each type of node\n",
    "- even though you tune for one type of node, it should run without crashing on any node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For those seeking peak GPU performance, you can also control the \"grid size\" (number of thread blocks)\n",
    "\n",
    "Add, e.g., `Gs=15` to the `run_fma_prof` and `run_fma_prof_opt` targets\n",
    "\n",
    "This will let you experiment with ILP vs. TLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Review: Little's Law\n",
    "\n",
    "$\\Huge L = \\lambda W$\n",
    "\n",
    "#### - $L$: amount of $x$ in a system.\n",
    "#### - $\\lambda$: arrival rate, $x$ / sec.\n",
    "#### - $W$: time spent in the system (sec).\n",
    "\n",
    "- Note that it has dimensions $x$, whatever it is we're trying to count.\n",
    "- For pipelines, we often think of $\\lambda$ as $x$ / cycle and $W$ as length of the pipeline in cycles.  Every unit but $x$ cancels out, so we can take whichever form is more convenient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example from last time: how many independent FMAs are needed for peak flop/s?\n",
    "\n",
    "#### - $W$: depth of the pipeline\n",
    "#### - $\\lambda$: arrival rate = # FPUs * # FMAs in a *vectorized* instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[[Intel's intrisics guide](https://software.intel.com/sites/landingpage/IntrinsicsGuide/)]\n",
    "\n",
    "![vfmadd132ps](./images/intel-intrinsics.png)\n",
    "\n",
    "latency = $W$, CPI (cycles per instruction) = 1 / $\\lambda_v$, where $\\lambda_v$ = throughput for *vectorized* FMAs.  Multiply by vector width (see operation pseudocode) to get full $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Question from last time:\n",
    "\n",
    "#### - What is the latency of FMA on the GPUs?  How could we estimate it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Quick Review: CPUs  (Hosts)\n",
    "\n",
    "#### - One set of instructions per thread\n",
    "#### - OS schedules threads (software multithreading), can *migrate* them between cores\n",
    "#### - x86-64 (AVX2) instruction set has 32 256-bit vector registers per threads\n",
    "#### - Parallelism via software multithread, hardware multithreading, superscalar execution, vectorization (**SIMD**)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Quick Review: GPUs (Devices)\n",
    "\n",
    "(See the nice illustrations from Prof. Vuduc's [slides](http://vuduc.org/cse6230/slides/cse6230-fa14--05-cuda.pdf), starting on slide 27)\n",
    "\n",
    "#### - A *compute kernel* is a task that the host assigns to the device in a kernel launch\n",
    "\n",
    "```c++\n",
    "\n",
    "solveForX<<<ThreadsPerBlock,BlocksPerGrid>>>(A,x,y);\n",
    "```\n",
    "\n",
    "- Proceeds asynchronously from the host until the host requires the results\n",
    "\n",
    "#### - The task is broken down into a **grid** of *independent* thread blocks\n",
    "\n",
    "- The host has no control over which thread blocks are assigned where and in what order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### - Each thread block is assigned to a streaming multiprocessor (SM), where it stays\n",
    "\n",
    "- A SM may be assigned multiple thread blocks\n",
    "\n",
    "#### - The SM breaks down the thread blocks into **warps** (32 threads): a warp shares an instruction set, so all (non-divergent) instructions are vectorized\n",
    "#### - The SM issues instructions from multiple warps per cycle (sometimes multiple instructions per warp per cycle) \n",
    "#### - When a warp is stalled, another is scheduled\n",
    "#### - Avoid: thread divergence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Using `nvprof` to estimate the FMA latency\n",
    "\n",
    "`nvprof` is a performance analysis tool like `perf` and `gprof` combined for the GPU\n",
    "\n",
    "It can be inserted before a program in the same way as `perf`.  Here's the standard invocation of our\n",
    "program from the first assignment.  First info about what kind of GPU I'm using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug 28 07:02:06 2018       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 367.55                 Driver Version: 367.55                    |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  On   | 0000:81:00.0     Off |                    0 |\n",
      "| N/A   24C    P0    25W / 250W |      0MiB / 16276MiB |      0%   E. Process |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID  Type  Process name                               Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc -ccbin=icpc -Xcompiler '-fPIC'  -dc -o fma_loop_dev.o fma_loop_dev.cu\n",
      "nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "nvcc -ccbin=icpc -Xcompiler '-fPIC' -dlink  fma_cuda.o fma_loop_dev.o -o fma_cuda_link.o\n",
      "nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "icpc -qopenmp -shared -Wl,-soname,libfma_cuda.so -o libfma_cuda.so fma_cuda_link.o fma_cuda.o fma_loop_dev.o -L/usr/local/pacerepov1/cuda/8.0.44/lib64 -Wl,-rpath,/usr/local/pacerepov1/cuda/8.0.44/lib64 -lcudart\n",
      "icpc -qopenmp -o fma_prof fma_prof.o fma_omp.o fma_loop_host.o libfma_cuda.so -Wl,-rpath,.\n",
      "OMP_PROC_BIND=spread OMP_NUM_THREADS=8  ./fma_prof 0 114688 1024 112 250000 0.5 3.0\n",
      "[./fma_prof] Nh = 0, Nd = 114688, T = 250000, block size = 1024\n",
      "[./fma_prof]: 2.236700e-02 elapsed seconds\n",
      "[./fma_prof]: 57344000000 flops executed\n",
      "[./fma_prof]: 2.563777e+12 flop/s\n"
     ]
    }
   ],
   "source": [
    "cd $CSE6230_DIR/assignments/first-week-flops\n",
    "git checkout fma_loop_dev.cu\n",
    "make run_fma_prof Nh=0 Bs=1024 Gs=$((56*2)) Nd=$((1024*56*2)) T=250000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that we are using the maximum thread block size (1024) and making two times as many thread blocks as there are SMs (56), leading to 2048 threads per SM, the maximum **occupancy**.\n",
    "\n",
    "#### - **Occupancy**: the ratio of active warps in an SM (GPU) to the maximum number of warps per SM (GPU). \n",
    "\n",
    "A good measure of whether there is additional *thread-level parallelism* on the device.  Let's see if we can measure that.\n",
    "\n",
    "(Note: for this GPU, I could get closer to 100% occupancy with a block size of 512 and twice as many blocks.  Why?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              achieved_occupancy:  Ratio of the average active warps per active cycle to the maximum number of warps supported on a multiprocessor\n",
      "              achieved_occupancy:  Ratio of the average active warps per active cycle to the maximum number of warps supported on a multiprocessor\n"
     ]
    }
   ],
   "source": [
    "nvprof --query-metrics | grep occupancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OMP_PROC_BIND=spread OMP_NUM_THREADS=8 nvprof --metrics achieved_occupancy ./fma_prof 0 114688 1024 112 250000 0.5 3.0\n",
      "[./fma_prof] Nh = 0, Nd = 114688, T = 250000, block size = 1024\n",
      "==3858== NVPROF is profiling process 3858, command: ./fma_prof 0 114688 1024 112 250000 0.5 3.0\n",
      "[./fma_prof]: 4.061890e-02 elapsed seconds\n",
      "[./fma_prof]: 57344000000 flops executed\n",
      "[./fma_prof]: 1.411757e+12 flop/s\n",
      "==3858== Profiling application: ./fma_prof 0 114688 1024 112 250000 0.5 3.0\n",
      "==3858== Profiling result:\n",
      "==3858== Metric result:\n",
      "Invocations                               Metric Name                        Metric Description         Min         Max         Avg\n",
      "Device \"Tesla P100-PCIE-16GB (0)\"\n",
      "    Kernel: __nv_static_56__43_tmpxft_00007909_00000000_7_fma_cuda_cpp1_ii_3fd49640__Z14fma_initializeiPf\n",
      "          1                        achieved_occupancy                        Achieved Occupancy    0.831391    0.831391    0.831391\n",
      "    Kernel: fma_loop_dev(int, int, float*, float, float)\n",
      "          1                        achieved_occupancy                        Achieved Occupancy    0.768974    0.768974    0.768974\n"
     ]
    }
   ],
   "source": [
    "make run_fma_prof Nh=0 Bs=1024 Gs=$((56*2)) Nd=$((1024*56*2)) T=250000 PERF=\"nvprof --metrics achieved_occupancy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Okay, but let's look at the measure that we're actually interested, the percentag of peak flop/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              flop_hp_efficiency:  Ratio of achieved to peak half-precision floating-point operations\n",
      "              flop_sp_efficiency:  Ratio of achieved to peak single-precision floating-point operations\n",
      "              flop_dp_efficiency:  Ratio of achieved to peak double-precision floating-point operations\n",
      "              flop_hp_efficiency:  Ratio of achieved to peak half-precision floating-point operations\n",
      "              flop_sp_efficiency:  Ratio of achieved to peak single-precision floating-point operations\n",
      "              flop_dp_efficiency:  Ratio of achieved to peak double-precision floating-point operations\n"
     ]
    }
   ],
   "source": [
    "nvprof --query-metrics | grep flop | grep peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OMP_PROC_BIND=spread OMP_NUM_THREADS=8 nvprof --metrics flop_sp_efficiency ./fma_prof 0 114688 1024 112 250000 0.5 3.0\n",
      "[./fma_prof] Nh = 0, Nd = 114688, T = 250000, block size = 1024\n",
      "==4217== NVPROF is profiling process 4217, command: ./fma_prof 0 114688 1024 112 250000 0.5 3.0\n",
      "==4217== Some kernel(s) will be replayed on device 0 in order to collect all events/metrics.\n",
      "==4217== Replaying kernel \"__nv_static_56__43_tmpxft_00007909_00000000_7_fma_cuda_cpp1_ii_3fd49640__Z14fma_initializeiPf\" (done)           \n",
      "==4217== Replaying kernel \"fma_loop_dev(int, int, float*, float, float)\" (done)           \n",
      "[./fma_prof]: 1.543440e+00 elapsed seconds\n",
      "[./fma_prof]: 57344000000 flops executed\n",
      "[./fma_prof]: 3.715337e+10 flop/s\n",
      "==4217== Profiling application: ./fma_prof 0 114688 1024 112 250000 0.5 3.0\n",
      "==4217== Profiling result:\n",
      "==4217== Metric result:\n",
      "Invocations                               Metric Name                        Metric Description         Min         Max         Avg\n",
      "Device \"Tesla P100-PCIE-16GB (0)\"\n",
      "    Kernel: __nv_static_56__43_tmpxft_00007909_00000000_7_fma_cuda_cpp1_ii_3fd49640__Z14fma_initializeiPf\n",
      "          1                        flop_sp_efficiency              FLOP Efficiency(Peak Single)       0.00%       0.00%       0.00%\n",
      "    Kernel: fma_loop_dev(int, int, float*, float, float)\n",
      "          1                        flop_sp_efficiency              FLOP Efficiency(Peak Single)      48.66%      48.66%      48.66%\n"
     ]
    }
   ],
   "source": [
    "make run_fma_prof Nh=0 Bs=1024 Gs=$((56*2)) Nd=$((1024*56*2)) T=250000 PERF=\"nvprof --metrics flop_sp_efficiency\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So while we have good occupancy, we are acheiving only ~50% of peak flop/s.\n",
    "\n",
    "First, let's unroll things just a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff --git a/assignments/first-week-flops/fma_loop_dev.cu b/assignments/first-week-flops/fma_loop_dev.cu\n",
      "index c603504..d556288 100644\n",
      "--- a/assignments/first-week-flops/fma_loop_dev.cu\n",
      "+++ b/assignments/first-week-flops/fma_loop_dev.cu\n",
      "@@ -9,6 +9,7 @@ fma_loop_dev (int N, int T, float *a, float b, float c)\n",
      "   int num_threads = gridDim.x * blockDim.x;\n",
      " \n",
      "   for (int i = my_thread; i < N; i+= num_threads) {\n",
      "+#pragma unroll 64\n",
      "     for (int j = 0; j < T; j++) {\n",
      "       a[i] = a[i] * b + c;\n",
      "     }\n"
     ]
    }
   ],
   "source": [
    "sed -i -e '/for (int j/i #pragma unroll 64' fma_loop_dev.cu\n",
    "git diff fma_loop_dev.cu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc -ccbin=icpc -Xcompiler '-fPIC'  -dc -o fma_loop_dev.o fma_loop_dev.cu\n",
      "nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "nvcc -ccbin=icpc -Xcompiler '-fPIC' -dlink  fma_cuda.o fma_loop_dev.o -o fma_cuda_link.o\n",
      "nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "icpc -qopenmp -shared -Wl,-soname,libfma_cuda.so -o libfma_cuda.so fma_cuda_link.o fma_cuda.o fma_loop_dev.o -L/usr/local/pacerepov1/cuda/8.0.44/lib64 -Wl,-rpath,/usr/local/pacerepov1/cuda/8.0.44/lib64 -lcudart\n",
      "icpc -qopenmp -o fma_prof fma_prof.o fma_omp.o fma_loop_host.o libfma_cuda.so -Wl,-rpath,.\n",
      "OMP_PROC_BIND=spread OMP_NUM_THREADS=8 nvprof --metrics flop_sp_efficiency ./fma_prof 0 114688 1024 112 250000 0.5 3.0\n",
      "[./fma_prof] Nh = 0, Nd = 114688, T = 250000, block size = 1024\n",
      "==4457== NVPROF is profiling process 4457, command: ./fma_prof 0 114688 1024 112 250000 0.5 3.0\n",
      "==4457== Some kernel(s) will be replayed on device 0 in order to collect all events/metrics.\n",
      "==4457== Replaying kernel \"__nv_static_56__43_tmpxft_00007909_00000000_7_fma_cuda_cpp1_ii_3fd49640__Z14fma_initializeiPf\" (done)           \n",
      "==4457== Replaying kernel \"fma_loop_dev(int, int, float*, float, float)\" (done)           \n",
      "[./fma_prof]: 2.253981e+00 elapsed seconds\n",
      "[./fma_prof]: 57344000000 flops executed\n",
      "[./fma_prof]: 2.544121e+10 flop/s\n",
      "==4457== Profiling application: ./fma_prof 0 114688 1024 112 250000 0.5 3.0\n",
      "==4457== Profiling result:\n",
      "==4457== Metric result:\n",
      "Invocations                               Metric Name                        Metric Description         Min         Max         Avg\n",
      "Device \"Tesla P100-PCIE-16GB (0)\"\n",
      "    Kernel: __nv_static_56__43_tmpxft_00007909_00000000_7_fma_cuda_cpp1_ii_3fd49640__Z14fma_initializeiPf\n",
      "          1                        flop_sp_efficiency              FLOP Efficiency(Peak Single)       0.00%       0.00%       0.00%\n",
      "    Kernel: fma_loop_dev(int, int, float*, float, float)\n",
      "          1                        flop_sp_efficiency              FLOP Efficiency(Peak Single)      90.24%      90.24%      90.24%\n"
     ]
    }
   ],
   "source": [
    "make run_fma_prof Nh=0 Bs=1024 Gs=$((56*2)) Nd=$((1024*56*2)) T=250000 PERF=\"nvprof --metrics flop_sp_efficiency\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now we're seeing about ~90% efficiency (thought it took a lot of unrolling!)\n",
    "\n",
    "Now, according to the chart from last time:\n",
    "\n",
    "![NVIDIA comparison](../processors/images/nvidia-table.png)\n",
    "\n",
    "There are 64 single precision FPUs per SM on this Pascal GPU.  If I keep the same number of thread blocks, but set each block size to $64 / 2= 32$ (1 warp!), then there will be one thread per FPU.\n",
    "\n",
    "Because each thread's computation as a loop dependency (iteration $i + 1$ cannot start until iteration $i$ has completed), we should only be issuing one FMA at the rate that an operation can traverse the pipeline.  Let's see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OMP_PROC_BIND=spread OMP_NUM_THREADS=8 nvprof --metrics flop_sp_efficiency --metrics achieved_occupancy ./fma_prof 0 3584 32 112 8000000 0.5 3.0\n",
      "[./fma_prof] Nh = 0, Nd = 3584, T = 8000000, block size = 32\n",
      "==5138== NVPROF is profiling process 5138, command: ./fma_prof 0 3584 32 112 8000000 0.5 3.0\n",
      "==5138== Some kernel(s) will be replayed on device 0 in order to collect all events/metrics.\n",
      "==5138== Replaying kernel \"__nv_static_56__43_tmpxft_00007909_00000000_7_fma_cuda_cpp1_ii_3fd49640__Z14fma_initializeiPf\" (done)           \n",
      "==5138== Replaying kernel \"fma_loop_dev(int, int, float*, float, float)\" (done)           \n",
      "[./fma_prof]: 9.380638e+00 elapsed seconds\n",
      "[./fma_prof]: 57344000000 flops executed\n",
      "[./fma_prof]: 6.113017e+09 flop/s\n",
      "==5138== Profiling application: ./fma_prof 0 3584 32 112 8000000 0.5 3.0\n",
      "==5138== Profiling result:\n",
      "==5138== Metric result:\n",
      "Invocations                               Metric Name                        Metric Description         Min         Max         Avg\n",
      "Device \"Tesla P100-PCIE-16GB (0)\"\n",
      "    Kernel: __nv_static_56__43_tmpxft_00007909_00000000_7_fma_cuda_cpp1_ii_3fd49640__Z14fma_initializeiPf\n",
      "          1                        flop_sp_efficiency              FLOP Efficiency(Peak Single)       0.00%       0.00%       0.00%\n",
      "          1                        achieved_occupancy                        Achieved Occupancy    0.454907    0.454907    0.454907\n",
      "    Kernel: fma_loop_dev(int, int, float*, float, float)\n",
      "          1                        flop_sp_efficiency              FLOP Efficiency(Peak Single)      15.99%      15.99%      15.99%\n",
      "          1                        achieved_occupancy                        Achieved Occupancy    0.031250    0.031250    0.031250\n"
     ]
    }
   ],
   "source": [
    "make run_fma_prof Nh=0 Bs=32 Gs=$((56*2)) Nd=$((32*56*2)) T=8000000 PERF=\"nvprof --metrics flop_sp_efficiency --metrics achieved_occupancy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our achieved occupancy is ~1/32 = 32 / 1024, which makes sense.  Our efficiency is ~16%, so our estimated pipeline depth is **$1 / 0.16 \\approx 6$**.\n",
    "\n",
    "You could repeat this for the Kepler GPUs if you wanted, but the number would be quite different.\n",
    "As the chart above shows, the design of NVIDIA GPUs is not stable: Kepler has a few large SMs; Pascal has a lot of smaller SMs; Maxwell (not available on pace-ice) is in the middle, but almost completely abandons double precision arithmetic, which makes me sad :(."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
